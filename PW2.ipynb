{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ubaldinho/Hello_World/blob/main/PW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "b303a462f93006ad"
      },
      "cell_type": "markdown",
      "source": [
        "# Practical Work : Secure Federated Learning"
      ],
      "id": "b303a462f93006ad"
    },
    {
      "metadata": {
        "id": "69f1b9298f08bcd4"
      },
      "cell_type": "markdown",
      "source": [
        "Federated Learning (FL) is a machine learning framework that enables $K \\in \\mathbb{N}^*$ participants to collaboratively train a model $M^G$ across $R$ rounds of exchange while maintaining the privacy of their data $D^k$. In the client‚Äìserver model of FL, the server initializes the global model $M^G_0$. At each round $t$, the global model $M^G_t$ is distributed to a subset $S_t \\subseteq {1, \\dots, K}$ consisting of $C \\times K$ randomly selected clients, where $C \\in (0,1]$. Each client $k \\in S_t$ trains the model locally using its private dataset $D^k$ and sends its updated model $M_{t+1}^k$ back to the server. The server then aggregates these updates to construct the new global model $M_{t+1}^G$. This process repeats until $R$ rounds are completed ($t = R$)."
      ],
      "id": "69f1b9298f08bcd4"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tenseal"
      ],
      "metadata": {
        "id": "anClfl8jkUK4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc2b1f4-7cc8-4a7c-ace5-0c6477e242c0"
      },
      "id": "anClfl8jkUK4",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tenseal in /usr/local/lib/python3.12/dist-packages (0.3.16)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "182723512d0fe76a"
      },
      "cell_type": "code",
      "source": [
        "from src.train import train\n",
        "from src.data_splitter import data_splitter\n",
        "from src.metric import accuracy\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from copy import deepcopy\n",
        "from torch import optim\n",
        "\n",
        "import tenseal as ts"
      ],
      "id": "182723512d0fe76a",
      "outputs": [],
      "execution_count": 2
    },
    {
      "metadata": {
        "id": "280ccaf1937447b"
      },
      "cell_type": "markdown",
      "source": [
        "## Understanding the Client-Server Algorithm"
      ],
      "id": "280ccaf1937447b"
    },
    {
      "metadata": {
        "id": "596dec5faec6bfb6"
      },
      "cell_type": "markdown",
      "source": [
        "In this section, we implement the Federated Learning algorithm using FedAvg. We use the CIFAR-10 dataset and the ResNet-18 model. The server model is trained for 44 rounds, after which one round of training is performed on the clients.\n"
      ],
      "id": "596dec5faec6bfb6"
    },
    {
      "metadata": {
        "id": "c3a1f7642f29247a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa3726a-f73f-4f41-f64a-a22938d2ca0b"
      },
      "cell_type": "code",
      "source": [
        "train_loaders, size, test_loader = data_splitter(\"CIFAR10\", 5 ) # TODO (a) : Load the data using the data_splitter function"
      ],
      "id": "c3a1f7642f29247a",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Dataset :  CIFAR10 \n",
            "\n",
            "Size of the train set for each client : 2000\n",
            "Size of the test set : 10000 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "metadata": {
        "id": "93473d1cb9fc2d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed3ff004-d756-436e-d096-8d6099543ce1"
      },
      "cell_type": "code",
      "source": [
        "server_44 = torch.hub.load('pytorch/vision', 'resnet18', weights=None) # Load the model architecture\n",
        "server_44.fc = nn.Linear(server_44.fc.in_features, 10)\n",
        "server_44.load_state_dict(torch.load(\"/content/model_cifar10_fl.pth\", map_location=torch.device('cpu')) ) # TODO (b) : Load the model weights of the server at round 44\n",
        "#server_44.to(\"cuda\")\n",
        "print(\"Server model at round 44 loaded\")"
      ],
      "id": "93473d1cb9fc2d2",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server model at round 44 loaded\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "metadata": {
        "id": "5f7df5b3f1ef146d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07da6edc-5e59-46ea-c02a-226ea875d76b"
      },
      "cell_type": "code",
      "source": [
        "accuracy(server_44, test_loader)"
      ],
      "id": "5f7df5b3f1ef146d",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.594, 1.167)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "execution_count": 5
    },
    {
      "metadata": {
        "id": "919734bca89aac23"
      },
      "cell_type": "markdown",
      "source": [
        "In the next cell, we define five clients by copying the server model."
      ],
      "id": "919734bca89aac23"
    },
    {
      "metadata": {
        "id": "941b4fc1a1bb2cd"
      },
      "cell_type": "code",
      "source": [
        "clients = []\n",
        "for i in range(5):\n",
        "    client = deepcopy(server_44)\n",
        "    clients.append(client)"
      ],
      "id": "941b4fc1a1bb2cd",
      "outputs": [],
      "execution_count": 6
    },
    {
      "metadata": {
        "id": "c27761a07eb86d65"
      },
      "cell_type": "markdown",
      "source": [
        "Before launching the FL algorithm, we need to define the FedAvg function, which is defined as follows:\n",
        "$$ W^G_{t+1} = \\sum_{k \\in S_t} \\frac{n_k}{n} W^k_{t+1} $$\n",
        "where $W^k_{t+1}$ denotes the weights of client $k$ at round $t+1$, $n_k$ is the size of client $k$‚Äôs dataset, and $n$ is the total size of the datasets of all clients."
      ],
      "id": "c27761a07eb86d65"
    },
    {
      "metadata": {
        "id": "ab08460d48f187"
      },
      "cell_type": "code",
      "source": [
        "def fed_avg(server, clients):\n",
        "    with torch.no_grad():\n",
        "        server_next = deepcopy(server)\n",
        "        server_dict = server_next.state_dict()\n",
        "        for name_server in server_dict.keys():\n",
        "            server_dict[name_server].zero_()\n",
        "            for client in clients:\n",
        "                if client.state_dict()[name_server].dtype is torch.long:\n",
        "                    weight = (\n",
        "                        1 / len(clients) # todo (d) : Compute the weight of the client's model\n",
        "                    ) * client.state_dict()[name_server].clone().detach()\n",
        "                    weight = weight.long()\n",
        "\n",
        "                else:\n",
        "                    weight = (\n",
        "                       1 / len(clients) # todo (d) : Compute the weight of the client's model\n",
        "                    ) * client.state_dict()[name_server].clone().detach()\n",
        "\n",
        "                server_dict[name_server].add_(weight)\n",
        "    return server_next"
      ],
      "id": "ab08460d48f187",
      "outputs": [],
      "execution_count": 7
    },
    {
      "metadata": {
        "id": "b17301905822262d"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we can launch the FL algorithm by performing one round of training on the clients."
      ],
      "id": "b17301905822262d"
    },
    {
      "metadata": {
        "id": "f8c053d55bbed546",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e838791-0d99-4b5a-e49f-4a31ef127a06"
      },
      "cell_type": "code",
      "source": [
        "def one_round():\n",
        "    clients = []\n",
        "    for i in range(5):\n",
        "        print(\"Local Training on client\", i)\n",
        "        client = deepcopy(server_44)\n",
        "        clients.append(client.train()) # TODO (d) : Train the client's model using the train function and store it in the clients list\n",
        "    server_next = fed_avg(server_44, clients) # TODO (d) : Aggregate the client's model using the FedAvg algorithm\n",
        "    print(accuracy(server_next, test_loader) ) # TODO (e) : Evaluate the accuracy of the server model after aggregation\n",
        "\n",
        "one_round()"
      ],
      "id": "f8c053d55bbed546",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local Training on client 0\n",
            "Local Training on client 1\n",
            "Local Training on client 2\n",
            "Local Training on client 3\n",
            "Local Training on client 4\n",
            "(0.594, 1.167)\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "metadata": {
        "id": "7f01dd10ca70985e"
      },
      "cell_type": "markdown",
      "source": [
        "## Secure Aggregation using TenSEAL"
      ],
      "id": "7f01dd10ca70985e"
    },
    {
      "metadata": {
        "id": "c8726b2196df0a53"
      },
      "cell_type": "markdown",
      "source": [
        "In this section, we implement secure aggregation using the TenSEAL library. We use the CKKS scheme to encrypt the last layer of each client‚Äôs model and then aggregate the encrypted layers. Finally, we decrypt the aggregated layer to obtain the final result."
      ],
      "id": "c8726b2196df0a53"
    },
    {
      "metadata": {
        "id": "46b77ee9c5e542cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12197a59-08b8-4e51-c416-fd223ee012a4"
      },
      "cell_type": "code",
      "source": [
        "server_44 = torch.hub.load('pytorch/vision', 'resnet18', weights=None) # Load the model architecture\n",
        "server_44.fc = nn.Linear(server_44.fc.in_features, 10)\n",
        "server_44.load_state_dict(torch.load(\"/content/model_cifar10_fl.pth\", map_location=torch.device('cpu')) ) # TODO (b) : Load the model weights of the server at round 44\n",
        "#server_44.to(\"cuda\")\n",
        "print(\"Server model at round 44 loaded\")"
      ],
      "id": "46b77ee9c5e542cf",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server model at round 44 loaded\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "metadata": {
        "id": "55748fafe9b20397"
      },
      "cell_type": "markdown",
      "source": [
        "First, we need to define the encryption context using the CKKS scheme."
      ],
      "id": "55748fafe9b20397"
    },
    {
      "metadata": {
        "id": "1575463ec066a2d8"
      },
      "cell_type": "code",
      "source": [
        "ctx = ts.context(ts.SCHEME_TYPE.CKKS, 8192, coeff_mod_bit_sizes=[60, 40, 40, 60])\n",
        "ctx.global_scale = pow(2, 40)\n",
        "ctx.generate_galois_keys()"
      ],
      "id": "1575463ec066a2d8",
      "outputs": [],
      "execution_count": 10
    },
    {
      "metadata": {
        "id": "7975a4383344c517"
      },
      "cell_type": "markdown",
      "source": [
        "Then, we define the function that encrypts the last layer of each client‚Äôs model."
      ],
      "id": "7975a4383344c517"
    },
    {
      "metadata": {
        "id": "c88b64d6346a0f8b"
      },
      "cell_type": "code",
      "source": [
        "def encrypt_last_layer(clients, ctx):\n",
        "    encrypted_last_layers = []\n",
        "    for i in range(5):\n",
        "        encrypted_last_layers.append(\n",
        "            ts.ckks_tensor(ctx, clients[i].fc.bias.cpu().detach().numpy()) # TODO (f) : Encrypt the last layer of the client's model\n",
        "        )\n",
        "        # TODO (f) : Encrypt the last layer of the client's model\n",
        "    return encrypted_last_layers"
      ],
      "id": "c88b64d6346a0f8b",
      "outputs": [],
      "execution_count": 11
    },
    {
      "metadata": {
        "id": "f6ea4b568751d06a"
      },
      "cell_type": "code",
      "source": [
        "encrypted_last_layers = encrypt_last_layer(clients, ctx)"
      ],
      "id": "f6ea4b568751d06a",
      "outputs": [],
      "execution_count": 12
    },
    {
      "metadata": {
        "id": "7f2078a84c303edd"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we can aggregate the encrypted last layers and decrypt the aggregated layer to obtain the final result."
      ],
      "id": "7f2078a84c303edd"
    },
    {
      "metadata": {
        "id": "a9a8077ab2a53e7b"
      },
      "cell_type": "code",
      "source": [
        "cli_coeff = 1 / len(clients)\n",
        "aggregated_encrypted_last_layers =  cli_coeff * encrypted_last_layers[0]\n",
        "for i in range(1, 5):\n",
        "    aggregated_encrypted_last_layers += cli_coeff * encrypted_last_layers[i]\n",
        "# TODO (g) : Aggregate the encrypted last layers"
      ],
      "id": "a9a8077ab2a53e7b",
      "outputs": [],
      "execution_count": 13
    },
    {
      "metadata": {
        "id": "d2e68c9104b9f5c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e9ba690-4fc5-4e31-fdfc-d8a5c6bfb0bd"
      },
      "cell_type": "code",
      "source": [
        "result = aggregated_encrypted_last_layers.decrypt().tolist()\n",
        "print(result)\n",
        "# TODO (h) : Decrypt the aggregated last layer and print the result"
      ],
      "id": "d2e68c9104b9f5c9",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.12019600956226988, 0.05193927470126821, 0.1844228005301616, 0.35058398792729095, 0.08478416257873579, -0.16556600546367298, -0.3123740961464586, 0.18795419684798487, -0.07198966570366853, -0.001764628803325899]\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "metadata": {
        "id": "8a9015f73807938b"
      },
      "cell_type": "markdown",
      "source": [
        "You can compare the result with the aggregation of the clients‚Äô models without encryption:"
      ],
      "id": "8a9015f73807938b"
    },
    {
      "metadata": {
        "id": "c78a58a0f6109788",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee1b3d3-f68b-49e6-ef5e-0a30e54b4bd3"
      },
      "cell_type": "code",
      "source": [
        "aggregated_last_layer = 0\n",
        "for i in range(5):\n",
        "    aggregated_last_layer += (1/5) * clients[i].fc.bias.cpu().detach()\n",
        "print(aggregated_last_layer)"
      ],
      "id": "c78a58a0f6109788",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.1202,  0.0519,  0.1844,  0.3506,  0.0848, -0.1656, -0.3124,  0.1880,\n",
            "        -0.0720, -0.0018])\n"
          ]
        }
      ],
      "execution_count": 15
    },
    {
      "metadata": {
        "id": "efd05af781ea40c7"
      },
      "cell_type": "markdown",
      "source": [
        "## Byzantine Attack"
      ],
      "id": "efd05af781ea40c7"
    },
    {
      "metadata": {
        "id": "8f0623eb66f94fa8"
      },
      "cell_type": "markdown",
      "source": [
        "In this section, we implement various Byzantine attacks that aim to compromise the federated learning process by sending malicious updates to the server."
      ],
      "id": "8f0623eb66f94fa8"
    },
    {
      "metadata": {
        "id": "9bca58be356b4ab4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce198997-daeb-46fd-a1b3-a1287e185219"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server model at round 44 loaded\n"
          ]
        }
      ],
      "execution_count": 16,
      "source": [
        "server_44 = torch.hub.load('pytorch/vision', 'resnet18', weights=None) # Load the model architecture\n",
        "server_44.fc = nn.Linear(server_44.fc.in_features, 10)\n",
        "server_44.load_state_dict(torch.load(\"/content/model_cifar10_fl.pth\", map_location=torch.device('cpu')) ) # TODO (b) : Load the model weights of the server at round 44\n",
        "#server_44.to(\"cuda\")\n",
        "print(\"Server model at round 44 loaded\")"
      ],
      "id": "9bca58be356b4ab4"
    },
    {
      "metadata": {
        "id": "a5856b23ef0c5b08"
      },
      "cell_type": "markdown",
      "source": [
        "The next cell contains all the Byzantine attacks that we implement.  \n",
        "The Byzantine attacks are defined as follows:\n",
        "\n",
        "- **Lazy Attack:** A client sends arbitrary values (e.g., random or malformed updates).  \n",
        "- **Same Attack:** A client sends identical values for all parameters.  \n",
        "- **Sign Attack:** A client multiplies all weights by a scalar $\\alpha$ (i.e., scales the model).  \n",
        "- **Noise Attack:** A client adds random noise to the weights (sampled from a chosen distribution, e.g., $\\mathcal{N}(0,\\sigma^2)$).  \n",
        "\n",
        "Finally, we define the filter (defense) that will be used to detect and"
      ],
      "id": "a5856b23ef0c5b08"
    },
    {
      "metadata": {
        "id": "90c32782665d449a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 17,
      "source": [
        "def BA_Lazy(server):\n",
        "    with torch.no_grad():\n",
        "        server_next = deepcopy(server)\n",
        "        server_dict = server_next.state_dict()\n",
        "        for name_server in server_dict.keys():\n",
        "            server_dict[name_server].zero_() # TODO (i) : Fill the server's model with a what you want\n",
        "    return server_next\n",
        "\n",
        "# Byzantine Attack\n",
        "def BA_Same(server):\n",
        "    with torch.no_grad():\n",
        "        server_next = deepcopy(server)\n",
        "        alpha = 100\n",
        "        server_dict = server_next.state_dict()\n",
        "        for name_server in server_dict.keys():\n",
        "            server_dict[name_server].fill_(alpha) # TODO (j) : Fill the server's model with the same value\n",
        "    return server_next\n",
        "\n",
        "def BA_Sign(server):\n",
        "    with torch.no_grad():\n",
        "        server_next = deepcopy(server)\n",
        "        server_dict = server_next.state_dict()\n",
        "        beta = 1000\n",
        "        for name_server in server_dict.keys():\n",
        "            server_dict[name_server].copy_(- server.state_dict()[name_server] * beta) # TODO (j) : Multiply all the weights by a value alpha\n",
        "    return server_next\n",
        "\n",
        "def BA_Noise(server):\n",
        "    with torch.no_grad():\n",
        "        server_next = deepcopy(server)\n",
        "        server_dict = server_next.state_dict()\n",
        "        mu, sigma = 0, 0.1 # TODO (j) : Define the mean and standard deviation of the noise\n",
        "        for name_server in server_dict.keys():\n",
        "            if server_dict[name_server].dtype is not torch.long:\n",
        "                noise = torch.rand_like(server.state_dict()[name_server]) * sigma + mu # TODO (j) : Add some noise to the weights\n",
        "                server_dict[name_server].add_(noise)\n",
        "                #server_dict[name_server].copy_(server.state_dict()[name_server] + noise)\n",
        "    return server_next\n",
        "\n",
        "def filter(client):\n",
        "    \"\"\"\n",
        "    Filtre pour d√©tecter les clients Byzantine\n",
        "    threshold: seuil pour d√©tecter les valeurs identiques\n",
        "    noise_threshold: seuil pour d√©tecter le bruit excessif\n",
        "    \"\"\"\n",
        "\n",
        "    threshold, noise_threshold = 0.1, 0.05\n",
        "    with torch.no_grad():\n",
        "        client_dict = client.state_dict()\n",
        "\n",
        "        # V√©rification Same Value Attack\n",
        "        for name, param in client_dict.items():\n",
        "            if param.numel() > 1:  # √âviter les scalaires\n",
        "                unique_values = torch.unique(param)\n",
        "                if len(unique_values) == 1:  # Toutes les valeurs identiques\n",
        "                    print(f\"‚ö†Ô∏è Same Value Attack d√©tect√© dans {name}\")\n",
        "                    return False\n",
        "        print(\"‚úÖ Pas de Same Value Attack d√©tect√©\")\n",
        "        # V√©rification Lazy Attack (tous z√©ros)\n",
        "        total_norm = 0\n",
        "        for param in client_dict.values():\n",
        "            param = param.float()\n",
        "            total_norm += param.norm().item()\n",
        "\n",
        "        if total_norm < threshold:  # Norme trop petite = mod√®le trop proche de z√©ro\n",
        "            print(f\"‚ö†Ô∏è Lazy Attack d√©tect√© (norme totale: {total_norm:.6f})\")\n",
        "            return False\n",
        "        print(\"‚úÖ Pas de Lazy Attack d√©tect√©\")\n",
        "\n",
        "\n",
        "    return True\n"
      ],
      "id": "90c32782665d449a"
    },
    {
      "metadata": {
        "id": "df42e298985fd72e"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 18,
      "source": [
        "def one_round_attack(server):\n",
        "    # Clients Side\n",
        "    clients = []\n",
        "    for i in range(5):\n",
        "        if i == 4:\n",
        "            print(\"Malicious Client\", i)\n",
        "            client = BA_Lazy(server) # TODO (k) : Apply the Byzantine Attack on the server\n",
        "        else:\n",
        "            print(\"Local Training on client\", i)\n",
        "            client = deepcopy(server)\n",
        "            train(client, train_loaders[i], test_loader, 1)\n",
        "        clients.append(client)\n",
        "\n",
        "    # Server Side\n",
        "    for i in range(5):\n",
        "        print(\"Client\", i)\n",
        "        clients[filter(clients[i])] # TODO (l) : Filter the client's model\n",
        "    server_next = fed_avg(server_44, clients)\n",
        "    print(\"Server Accuracy at round 45 \",accuracy(server_next, test_loader))"
      ],
      "id": "df42e298985fd72e"
    },
    {
      "metadata": {
        "id": "ba10ea5fa2cf91fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a0594dc-e20a-414a-d4dd-374d77ab695b"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local Training on client 0\n",
            "Epoch : 0\n",
            "(0.671, 1.115)\n",
            "Local Training on client 1\n",
            "Epoch : 0\n",
            "(0.67, 1.019)\n",
            "Local Training on client 2\n",
            "Epoch : 0\n",
            "(0.667, 1.182)\n",
            "Local Training on client 3\n",
            "Epoch : 0\n",
            "(0.669, 1.127)\n",
            "Malicious Client 4\n",
            "Client 0\n",
            "‚úÖ Pas de Same Value Attack d√©tect√©\n",
            "‚úÖ Pas de Lazy Attack d√©tect√©\n",
            "Client 1\n",
            "‚úÖ Pas de Same Value Attack d√©tect√©\n",
            "‚úÖ Pas de Lazy Attack d√©tect√©\n",
            "Client 2\n",
            "‚úÖ Pas de Same Value Attack d√©tect√©\n",
            "‚úÖ Pas de Lazy Attack d√©tect√©\n",
            "Client 3\n",
            "‚úÖ Pas de Same Value Attack d√©tect√©\n",
            "‚úÖ Pas de Lazy Attack d√©tect√©\n",
            "Client 4\n",
            "‚ö†Ô∏è Same Value Attack d√©tect√© dans conv1.weight\n",
            "Server Accuracy at round 45  (0.329, 1.907)\n"
          ]
        }
      ],
      "execution_count": 19,
      "source": [
        "one_round_attack(server_44)"
      ],
      "id": "ba10ea5fa2cf91fb"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#HOMEWORK 1\n",
        "def analyze_client_deviation(clients, i, last_layer_only=True):\n",
        "    \"\"\"\n",
        "    Analyse les √©carts des poids et biais du client i par rapport aux autres clients.\n",
        "    D√©tecte les anomalies de type :\n",
        "    - Diff√©rence absolue trop √©lev√©e\n",
        "    - Ratio absolu trop √©lev√© (indiquant une mise √† l'√©chelle ou inversion)\n",
        "\n",
        "    Args:\n",
        "        clients (list): Liste de mod√®les clients (instances de nn.Module)\n",
        "        i (int): Index du client √† analyser\n",
        "        last_layer_only (bool): Si True, analyse uniquement la derni√®re couche (nom contenant 'fc') ce qui est le mode le moins couteux mais tout aussi efficace\n",
        "\n",
        "    Returns:\n",
        "        bool: True si le client est conforme, False si des anomalies sont d√©tect√©es\n",
        "    \"\"\"\n",
        "\n",
        "    # Seuils de d√©tection\n",
        "    #DIFF_THRESHOLD = 0.1\n",
        "    RATIO_THRESHOLD = 2.0\n",
        "\n",
        "    target_dict = clients[i].state_dict()\n",
        "    other_dicts = [clients[j].state_dict() for j in range(len(clients)) if j != i]\n",
        "\n",
        "    suspicious_params = []\n",
        "\n",
        "    for name, param in target_dict.items():\n",
        "        if last_layer_only and 'fc' not in name:\n",
        "            continue  # ignorer les couches sauf la derni√®re\n",
        "\n",
        "        # Empiler les param√®tres des autres clients\n",
        "        others_tensor = torch.stack([other[name] for other in other_dicts])\n",
        "        mean_tensor = torch.mean(others_tensor, dim=0)\n",
        "\n",
        "        # Calcul des √©carts\n",
        "        abs_rel_ratio = torch.abs((param - mean_tensor) / (mean_tensor + 1e-8))  # √©viter division par z√©ro\n",
        "\n",
        "        # V√©rification des seuils\n",
        "        #if torch.any(abs_diff > DIFF_THRESHOLD):\n",
        "        #    suspicious_params.append((name, 'diff', abs_diff.max().item()))\n",
        "        if torch.any(abs_rel_ratio > RATIO_THRESHOLD):\n",
        "            suspicious_params.append((name, 'ratio', abs_rel_ratio.max().item()))\n",
        "\n",
        "    # Affichage des r√©sultats\n",
        "    if suspicious_params:\n",
        "        print(f\"‚ö†Ô∏è Client {i} pr√©sente des anomalies :\")\n",
        "        for name, typ, val in suspicious_params:\n",
        "            print(f\" - {name} d√©passe le seuil ({typ} = {val:.4f})\")\n",
        "        return False\n",
        "    else:\n",
        "        print(f\"‚úÖ Client {i} est conforme aux seuils d√©finis.\")\n",
        "        return True"
      ],
      "metadata": {
        "id": "mutGc48jCx0r"
      },
      "id": "mutGc48jCx0r",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Seuils pour la d√©tection de bruit\n",
        "STD_THRESHOLD = 0.1\n",
        "VAR_THRESHOLD = 0.1\n",
        "DISTANCE_THRESHOLD = 0.5\n",
        "COSINE_THRESHOLD = 0.4\n",
        "#HOMEWORK\n",
        "def detect_noise_attack(clients, i, last_layer_only=True):\n",
        "    \"\"\"\n",
        "    D√©tecte une attaque Byzantine de type bruit (Additive Noise Attack) en comparant\n",
        "    les mises √† jour du client i avec celles des autres clients.\n",
        "\n",
        "    M√©triques utilis√©es :\n",
        "    - √âcart-type et variance des diff√©rences\n",
        "    - Distance euclidienne\n",
        "    - Similarit√© cosinus\n",
        "\n",
        "    Args:\n",
        "        clients (list): Liste de mod√®les clients (instances de nn.Module)\n",
        "        i (int): Index du client √† analyser\n",
        "        last_layer_only (bool): Si True, analyse uniquement la derni√®re couche (nom contenant 'fc')\n",
        "\n",
        "    Returns:\n",
        "        bool: True si le client est conforme, False si bruit excessif d√©tect√©\n",
        "    \"\"\"\n",
        "    target_dict = clients[i].state_dict()\n",
        "    other_dicts = [clients[j].fc.bias for j in range(len(clients)) if j != i]\n",
        "\n",
        "    noisy_params = []\n",
        "\n",
        "    for name in target_dict.keys():\n",
        "        if last_layer_only and 'fc' not in name:\n",
        "            continue\n",
        "        param = clients[i].fc.bias\n",
        "\n",
        "        # Empiler les param√®tres des autres clients\n",
        "        others_tensor = torch.stack([other for other in other_dicts])\n",
        "        mean_tensor = torch.mean(others_tensor, dim=0)\n",
        "\n",
        "        delta = param - mean_tensor\n",
        "\n",
        "        # M√©triques statistiques\n",
        "        std_dev = torch.std(delta)\n",
        "        variance = torch.var(delta)\n",
        "        mean_val = torch.mean(delta) + 1e-8\n",
        "\n",
        "        # Distance euclidienne\n",
        "        euclidean_dist = torch.norm(delta)\n",
        "\n",
        "        # Similarit√© cosinus\n",
        "        cosine_sim = F.cosine_similarity(param.flatten(), mean_tensor.flatten(), dim=0)\n",
        "\n",
        "        if (\n",
        "            std_dev / mean_val > STD_THRESHOLD or\n",
        "            sqrt(variance) / mean_val > VAR_THRESHOLD or\n",
        "            euclidean_dist / mean_val > DISTANCE_THRESHOLD or\n",
        "            cosine_sim < COSINE_THRESHOLD\n",
        "        ):\n",
        "            noisy_params.append((name, std_dev.item(), variance.item(), euclidean_dist.item(), cosine_sim.item()))\n",
        "\n",
        "    if noisy_params:\n",
        "        print(f\"‚ö†Ô∏è Client {i} pr√©sente des signes de Noise Attack :\")\n",
        "        for name, std, var, dist, cos in noisy_params:\n",
        "            print(f\" - {name} : std={std:.4f}, var={var:.4f}, dist={dist:.4f}, cos_sim={cos:.4f}\")\n",
        "        return False\n",
        "    else:\n",
        "        print(f\"‚úÖ Client {i} ne pr√©sente pas de bruit excessif.\")\n",
        "        return True"
      ],
      "metadata": {
        "id": "XCE5VprWI7t-"
      },
      "id": "XCE5VprWI7t-",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from math import sqrt\n",
        "# HOMEWORK\n",
        "def filter_all(clients, i):\n",
        "    \"\"\"\n",
        "    Filtre pour d√©tecter les clients Byzantine\n",
        "    \"\"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        client = clients[i]\n",
        "        client_dict = client.state_dict()\n",
        "\n",
        "        # V√©rification Sign Attack (valeurs extr√™mes)\n",
        "        analyze_client_deviation(clients, i)\n",
        "\n",
        "        #verfification de Noisy Attack\n",
        "        detect_noise_attack(clients, i)\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "Ktl72KTxpJGy"
      },
      "id": "Ktl72KTxpJGy",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_round_attack_ext(server, attack_fn, filter_fn = filter_all):\n",
        "    \"\"\"\n",
        "    Ex√©cute un round d'entra√Ænement f√©d√©r√© avec une attaque Byzantine sur un client.\n",
        "\n",
        "    Args:\n",
        "        server (nn.Module): Mod√®le global du serveur.\n",
        "        attack_fn (function): Fonction d'attaque Byzantine √† appliquer sur le client malveillant.\n",
        "        filter_fn (function): Fonction de filtrage pour d√©tecter les clients suspects.\n",
        "    \"\"\"\n",
        "    clients = []\n",
        "\n",
        "    # Phase client\n",
        "    for i in range(5):\n",
        "        print(f\"Client {i}\")\n",
        "        client = deepcopy(server)\n",
        "\n",
        "        if i == 4:\n",
        "            print(\"‚ö†Ô∏è Malicious Client\", i)\n",
        "            client = attack_fn(client)  # Appliquer l'attaque\n",
        "        else:\n",
        "            print(\"‚úÖ Local Training on client\", i)\n",
        "            train(client, train_loaders[i], test_loader, 1)\n",
        "\n",
        "        clients.append(client)\n",
        "\n",
        "    # Phase serveur : filtrage\n",
        "    filtered_clients = []\n",
        "    for i in range(5):\n",
        "        print(f\"üîç Analyse du client {i}\")\n",
        "        if filter_fn(clients, i):\n",
        "            filtered_clients.append(clients[i])\n",
        "        else:\n",
        "            print(f\"‚ùå Client {i} exclu pour comportement suspect\")\n",
        "\n",
        "    # Agr√©gation\n",
        "    server_next = fed_avg(server, filtered_clients)\n",
        "    print(\"üìä Server Accuracy at round 45:\", accuracy(server_next, test_loader))"
      ],
      "metadata": {
        "id": "Pb7BtDAzLI3f"
      },
      "id": "Pb7BtDAzLI3f",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_round_attack_ext(server_44, BA_Sign)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKwr37MWLlXp",
        "outputId": "c490760a-bdcc-466a-f3ec-4e9af09d80c8"
      },
      "id": "uKwr37MWLlXp",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 0\n",
            "‚úÖ Local Training on client 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.666, 1.104)\n",
            "Client 1\n",
            "‚úÖ Local Training on client 1\n",
            "Epoch : 0\n",
            "(0.664, 1.145)\n",
            "Client 2\n",
            "‚úÖ Local Training on client 2\n",
            "Epoch : 0\n",
            "(0.663, 1.247)\n",
            "Client 3\n",
            "‚úÖ Local Training on client 3\n",
            "Epoch : 0\n",
            "(0.668, 1.5)\n",
            "Client 4\n",
            "‚ö†Ô∏è Malicious Client 4\n",
            "üîç Analyse du client 0\n",
            "‚úÖ Client 0 est conforme aux seuils d√©finis.\n",
            "‚ö†Ô∏è Client 0 pr√©sente des signes de Noise Attack :\n",
            " - fc.weight : std=48.8703, var=2388.3032, dist=147.3621, cos_sim=-1.0000\n",
            " - fc.bias : std=48.8703, var=2388.3032, dist=147.3621, cos_sim=-1.0000\n",
            "üîç Analyse du client 1\n",
            "‚úÖ Client 1 est conforme aux seuils d√©finis.\n",
            "‚ö†Ô∏è Client 1 pr√©sente des signes de Noise Attack :\n",
            " - fc.weight : std=48.8702, var=2388.2971, dist=147.3619, cos_sim=-1.0000\n",
            " - fc.bias : std=48.8702, var=2388.2971, dist=147.3619, cos_sim=-1.0000\n",
            "üîç Analyse du client 2\n",
            "‚úÖ Client 2 est conforme aux seuils d√©finis.\n",
            "‚ö†Ô∏è Client 2 pr√©sente des signes de Noise Attack :\n",
            " - fc.weight : std=48.8704, var=2388.3201, dist=147.3626, cos_sim=-1.0000\n",
            " - fc.bias : std=48.8704, var=2388.3201, dist=147.3626, cos_sim=-1.0000\n",
            "üîç Analyse du client 3\n",
            "‚úÖ Client 3 est conforme aux seuils d√©finis.\n",
            "‚ö†Ô∏è Client 3 pr√©sente des signes de Noise Attack :\n",
            " - fc.weight : std=48.8708, var=2388.3540, dist=147.3636, cos_sim=-1.0000\n",
            " - fc.bias : std=48.8708, var=2388.3540, dist=147.3636, cos_sim=-1.0000\n",
            "üîç Analyse du client 4\n",
            "‚ö†Ô∏è Client 4 pr√©sente des anomalies :\n",
            " - fc.weight d√©passe le seuil (ratio = 122265.6562)\n",
            " - fc.bias d√©passe le seuil (ratio = 1030.5281)\n",
            "‚ö†Ô∏è Client 4 pr√©sente des signes de Noise Attack :\n",
            " - fc.weight : std=195.4817, var=38213.0977, dist=589.4502, cos_sim=-1.0000\n",
            " - fc.bias : std=195.4817, var=38213.0977, dist=589.4502, cos_sim=-1.0000\n",
            "üìä Server Accuracy at round 45: (0.1, nan)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_round_attack_ext(server_44, BA_Noise)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWJbNK_RLdmV",
        "outputId": "a9597da6-c54a-4776-b0a2-59ebcb34a501"
      },
      "id": "JWJbNK_RLdmV",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 0\n",
            "‚úÖ Local Training on client 0\n",
            "Epoch : 0\n",
            "(0.662, 1.351)\n",
            "Client 1\n",
            "‚úÖ Local Training on client 1\n",
            "Epoch : 0\n",
            "(0.66, 1.226)\n",
            "Client 2\n",
            "‚úÖ Local Training on client 2\n",
            "Epoch : 0\n",
            "(0.666, 1.171)\n",
            "Client 3\n",
            "‚úÖ Local Training on client 3\n",
            "Epoch : 0\n",
            "(0.669, 1.235)\n",
            "Client 4\n",
            "‚ö†Ô∏è Malicious Client 4\n",
            "üîç Analyse du client 0\n",
            "‚ö†Ô∏è Client 0 pr√©sente des anomalies :\n",
            " - fc.weight d√©passe le seuil (ratio = 3153.5251)\n",
            "‚úÖ Client 0 ne pr√©sente pas de bruit excessif.\n",
            "üîç Analyse du client 1\n",
            "‚ö†Ô∏è Client 1 pr√©sente des anomalies :\n",
            " - fc.weight d√©passe le seuil (ratio = 13001.1006)\n",
            "‚úÖ Client 1 ne pr√©sente pas de bruit excessif.\n",
            "üîç Analyse du client 2\n",
            "‚ö†Ô∏è Client 2 pr√©sente des anomalies :\n",
            " - fc.weight d√©passe le seuil (ratio = 878.4620)\n",
            "‚úÖ Client 2 ne pr√©sente pas de bruit excessif.\n",
            "üîç Analyse du client 3\n",
            "‚ö†Ô∏è Client 3 pr√©sente des anomalies :\n",
            " - fc.weight d√©passe le seuil (ratio = 9969.5381)\n",
            "‚úÖ Client 3 ne pr√©sente pas de bruit excessif.\n",
            "üîç Analyse du client 4\n",
            "‚ö†Ô∏è Client 4 pr√©sente des anomalies :\n",
            " - fc.weight d√©passe le seuil (ratio = 4533.8330)\n",
            " - fc.bias d√©passe le seuil (ratio = 23.7995)\n",
            "‚ö†Ô∏è Client 4 pr√©sente des signes de Noise Attack :\n",
            " - fc.weight : std=0.0239, var=0.0006, dist=0.2312, cos_sim=0.9354\n",
            " - fc.bias : std=0.0239, var=0.0006, dist=0.2312, cos_sim=0.9354\n",
            "üìä Server Accuracy at round 45: (0.281, 3.465)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "4046c70f5e0cdd8c"
      },
      "cell_type": "markdown",
      "source": [
        "## Protect the Model's IP using Watermarking"
      ],
      "id": "4046c70f5e0cdd8c"
    },
    {
      "metadata": {
        "id": "65cb6442c8d6f2a8"
      },
      "cell_type": "markdown",
      "source": [
        "In this section, we implement the model watermarking technique defined by Uchida et al. To simplify the implementation, we assume that the model is watermarked by a single client."
      ],
      "id": "65cb6442c8d6f2a8"
    },
    {
      "metadata": {
        "id": "a65821e4f7128c27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea8cb600-5bcf-4085-9760-5ebff74ed68d"
      },
      "cell_type": "code",
      "source": [
        "# Model Watermarking\n",
        "server_44 = torch.hub.load('pytorch/vision', 'resnet18', weights=None)  # Load the model architecture\n",
        "server_44.fc = nn.Linear(server_44.fc.in_features, 10)\n",
        "server_44.load_state_dict(torch.load(\"/content/model_cifar10_fl.pth\", map_location=torch.device('cpu')) ) # TODO (b) : Load the model weights of the server at round 44\n",
        "#server_44.to(\"cuda\")\n",
        "print(\"Server model at round 44 loaded\")"
      ],
      "id": "a65821e4f7128c27",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server model at round 44 loaded\n"
          ]
        }
      ],
      "execution_count": 34
    },
    {
      "metadata": {
        "id": "9fced481d37982b4"
      },
      "cell_type": "markdown",
      "source": [
        "Before watermarking the model, we examine the limitation of a common method used to evaluate whether two models are identical in open-source platforms. In the next cells, we compute the hash of the last layer of the server model."
      ],
      "id": "9fced481d37982b4"
    },
    {
      "metadata": {
        "id": "220bafaf75022f37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc6c13a-a5e9-4942-ac66-a4cb720e90a5"
      },
      "cell_type": "code",
      "source": [
        "server_44.fc.bias"
      ],
      "id": "220bafaf75022f37",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([-0.1202,  0.0519,  0.1844,  0.3506,  0.0848, -0.1656, -0.3124,  0.1880,\n",
              "        -0.0720, -0.0018], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "execution_count": 35
    },
    {
      "metadata": {
        "id": "699101bffee40a3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2288c26e-8a3d-41aa-961c-6c6692d701eb"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.594, 1.167)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "execution_count": 36,
      "source": [
        "accuracy(server_44, test_loader)"
      ],
      "id": "699101bffee40a3a"
    },
    {
      "metadata": {
        "id": "babf7ad095202ba3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8643dca-6fdd-4630-c345-6cc05ccc68b1"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.12019599229097366 0.05193926766514778 0.18442277610301971 0.3505839407444 0.08478415012359619 -0.16556598246097565 -0.3123740553855896 0.18795417249202728 -0.07198965549468994 -0.0017646284541115165 \n",
            "Hash of the layer : 5858509149233719191\n"
          ]
        }
      ],
      "execution_count": 37,
      "source": [
        "tensor_to_str = ''.join(str(x.item())+\" \" for x in server_44.fc.bias)\n",
        "print(tensor_to_str)\n",
        "print(\"Hash of the layer :\", hash(tensor_to_str))"
      ],
      "id": "babf7ad095202ba3"
    },
    {
      "metadata": {
        "id": "6a3bec32ed2c3d45"
      },
      "cell_type": "markdown",
      "source": [
        "Let‚Äôs add a small perturbation to the last layer of the server model and then compute the hash again."
      ],
      "id": "6a3bec32ed2c3d45"
    },
    {
      "metadata": {
        "id": "dda0cd69c92de09a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 38,
      "source": [
        "with torch.no_grad():\n",
        "    server_44.fc.bias.add_(1e-3)"
      ],
      "id": "dda0cd69c92de09a"
    },
    {
      "metadata": {
        "id": "207bd7316dd57e05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97211545-f5be-4e3a-a16f-62d1f7adfeec"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.594, 1.167)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "execution_count": 39,
      "source": [
        "accuracy(server_44, test_loader)"
      ],
      "id": "207bd7316dd57e05"
    },
    {
      "metadata": {
        "id": "77746b30a1fd5348",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a7791cf-9b34-4865-82df-9fd40d3907e1"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.11919599026441574 0.052939265966415405 0.18542277812957764 0.35158392786979675 0.08578415215015411 -0.16456598043441772 -0.31137406826019287 0.1889541745185852 -0.07098965346813202 -0.0007646284066140652 \n",
            "Hash of the layer : 8435477227401369874\n"
          ]
        }
      ],
      "execution_count": 40,
      "source": [
        "tensor_to_str = ''.join(str(x.item())+\" \" for x in server_44.fc.bias)\n",
        "print(tensor_to_str)\n",
        "print(\"Hash of the layer :\", hash(tensor_to_str))"
      ],
      "id": "77746b30a1fd5348"
    },
    {
      "metadata": {
        "id": "601648462394ef16"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, the hash is different while the accuracy remains the same. This means that an attacker can bypass this simple method of verifying whether two models are identical."
      ],
      "id": "601648462394ef16"
    },
    {
      "metadata": {
        "id": "6bf3c58d18f415cd"
      },
      "cell_type": "markdown",
      "source": [
        "Now let‚Äôs implement the watermarking technique proposed by Uchida *et al.*  \n",
        "This technique consists of embedding a secret message in a layer using the following methodology:\n",
        "\n",
        "1. **Secret generation:** Generate a secret key $K$ and a message $b$.  \n",
        "2. **Parameter selection:** Select the parameters `\"fc.weight\"` and compute the mean along the columns to obtain a vector $w$.  \n",
        "3. **Projection:** Project the vector $w$, in which we want to embed $b$, using the secret key $K$ as follows:  \n",
        "   $$\n",
        "   y = K w\n",
        "   $$  \n",
        "4. **Extraction:** Apply the Sigmoid function to obtain the extracted message $b'$:  \n",
        "   $$\n",
        "   b' = \\sigma(y)\n",
        "   $$  \n",
        "5. **Loss computation:** Compute the binary cross-entropy loss between the extracted message $b'$ and the original message $b$:  \n",
        "   $$\n",
        "   L = \\text{BCELoss}(b', b)\n",
        "   $$"
      ],
      "id": "6bf3c58d18f415cd"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Cl√© secr√®te et message √† encoder\n",
        "secret_key = torch.randn((256, 512))  # X ‚àà ‚Ñù^{T√óM}\n",
        "message = torch.randint(2, (256,)).float()  # b ‚àà {0,1}^T\n",
        "\n",
        "def train_f(model, train_set, test_set, epoch_max):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion_watermark = nn.BCELoss()\n",
        "    alpha = 5e-1  # pond√©ration de la perte watermark\n",
        "\n",
        "    for epoch in range(epoch_max):\n",
        "        accumulate_loss = 0\n",
        "\n",
        "        for inputs, targets in train_set:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            outputs_predicted = model(inputs)\n",
        "            loss_main = criterion(outputs_predicted, targets)\n",
        "\n",
        "            # (n) Extraction du watermark\n",
        "            fc_weights = model.state_dict()['fc.weight']  # shape: [256, 512]\n",
        "            w_mean = torch.mean(fc_weights, dim=0)  # wÃÑ ‚àà ‚Ñù^256\n",
        "\n",
        "            y = torch.sigmoid(secret_key @ w_mean)  # y ‚àà ‚Ñù^256\n",
        "\n",
        "            # (o) Calcul de la perte watermark\n",
        "            loss_watermark = criterion_watermark(y, message)\n",
        "\n",
        "            # Perte totale\n",
        "            loss = loss_main + (alpha * loss_watermark)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            accumulate_loss += loss.item()\n",
        "\n",
        "        # (p) V√©rification du watermark : Bit Error Rate\n",
        "        with torch.no_grad():\n",
        "            w_mean = torch.mean(model.state_dict()['fc.weight'], dim=0)\n",
        "            y_extracted = torch.sigmoid(secret_key @ w_mean)\n",
        "            b_extracted = (y_extracted >= 0.5).float()\n",
        "            bit_error_rate = torch.mean(torch.abs(b_extracted - message)).item()\n",
        "\n",
        "        print(f\"Epoch : {epoch}\")\n",
        "        print(\"Bit Error Rate :\", bit_error_rate)"
      ],
      "metadata": {
        "id": "g7uiDCB2Q-0a"
      },
      "id": "g7uiDCB2Q-0a",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# G√©n√©ration du secret sur CPU\n",
        "secret_key = torch.randn((256, 512))\n",
        "message = torch.randint(2, (256,)).float()\n",
        "\n",
        "def train_g(model, train_set, test_set, epoch_max):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion_watermark = nn.BCELoss()  # TODO (o) : Binary Cross Entropy pour le watermark\n",
        "    alpha = 5e-1\n",
        "\n",
        "    for epoch in range(epoch_max):\n",
        "        accumulate_loss = 0\n",
        "\n",
        "        for inputs, outputs in train_set:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            outputs_predicted = model(inputs)\n",
        "            loss_main = criterion(outputs_predicted, outputs)\n",
        "\n",
        "            # TODO (n) : Extraction du message depuis le mod√®le\n",
        "            w = model.fc.weight.mean(dim=0)  # Moyenne des colonnes de fc.weight\n",
        "            y = torch.matmul(secret_key, w)  # Projection avec la cl√© secr√®te\n",
        "            extracted_message = torch.sigmoid(y)  # Application sigmoid\n",
        "\n",
        "            # TODO (o) : Calcul de la loss du watermark\n",
        "            loss_watermark = criterion_watermark(extracted_message, message)\n",
        "\n",
        "            loss = loss_main + (alpha * loss_watermark)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            accumulate_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch : {epoch}\")\n",
        "        print(accuracy(model, test_set))\n",
        "\n",
        "        # TODO (p) : Calcul du Bit Error Rate\n",
        "        predicted_bits = (extracted_message > 0.5).float()\n",
        "        bit_error_rate = (predicted_bits != message).float().mean().item()\n",
        "        print(\"Bit Error Rate : \", bit_error_rate)"
      ],
      "metadata": {
        "id": "MxmPGKNe0YrL"
      },
      "id": "MxmPGKNe0YrL",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = deepcopy(server_44)\n",
        "train_f(client, train_loaders[4], test_loader, 1)"
      ],
      "metadata": {
        "id": "1AHQgg-sSlON",
        "outputId": "fc609e4a-0e1d-48dd-e390-14afdb90b487",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1AHQgg-sSlON",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0\n",
            "Bit Error Rate : 0.5078125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = deepcopy(server_44)\n",
        "\n",
        "train_g(client, train_loaders[4], test_loader, 1)"
      ],
      "metadata": {
        "id": "b0k3itIafpT2",
        "outputId": "d869be59-f5c7-48e8-a1d2-8eb718a86ebf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "b0k3itIafpT2",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.664, 0.952)\n",
            "Bit Error Rate :  0.3671875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# √âtape 2 : Cr√©er une copie pour y int√©grer le watermark\n",
        "model_watermarked = deepcopy(server_44)\n",
        "\n",
        "# √âtape 3 : Cr√©er le trigger set\n",
        "def create_trigger_set(trigger_label=7, num_samples=100):\n",
        "    \"\"\"\n",
        "    G√©n√®re un ensemble d'images avec un motif constant et une √©tiquette fixe.\n",
        "    Ce set est utilis√© pour encoder un watermark dans le comportement du mod√®le.\n",
        "    \"\"\"\n",
        "    trigger_inputs = torch.ones((num_samples, 3, 32, 32)) * 0.5  # motif constant\n",
        "    trigger_targets = torch.full((num_samples,), trigger_label, dtype=torch.long)\n",
        "    return trigger_inputs, trigger_targets\n",
        "\n",
        "trigger_inputs, trigger_targets = create_trigger_set()\n",
        "\n",
        "# √âtape 4 : Entra√Æner le mod√®le avec le watermark\n",
        "def train_with_blackbox_watermark(model, train_loader, test_loader, trigger_inputs, trigger_targets, epoch_max):\n",
        "    \"\"\"\n",
        "    Entra√Æne le mod√®le √† r√©pondre au trigger set tout en conservant ses performances sur le test set.\n",
        "    \"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epoch_max):\n",
        "        model.train()\n",
        "        for inputs, targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Entra√Ænement sur le trigger set\n",
        "        optimizer.zero_grad()\n",
        "        outputs_trigger = model(trigger_inputs)\n",
        "        loss_trigger = criterion(outputs_trigger, trigger_targets)\n",
        "        loss_trigger.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch} - Trigger Loss: {loss_trigger.item():.4f}\")\n",
        "\n",
        "    # √âvaluation\n",
        "    def evaluate(loader):\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                outputs = model(inputs)\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                correct += (preds == targets).sum().item()\n",
        "                total += targets.size(0)\n",
        "        return correct / total\n",
        "\n",
        "    acc_test = evaluate(test_loader)\n",
        "    acc_trigger = torch.mean((torch.argmax(model(trigger_inputs), dim=1) == trigger_targets).float()).item()\n",
        "\n",
        "    print(\"‚úÖ Accuracy on test set:\", acc_test)\n",
        "    print(\"üîê Accuracy on trigger set:\", acc_trigger)\n",
        "\n",
        "# √âtape 5 : Appel de la fonction avec des loaders CIFAR-10\n",
        "train_with_blackbox_watermark(\n",
        "    model_watermarked,\n",
        "    train_loader=train_loaders[4],  # loader client\n",
        "    test_loader=test_loader,        # loader global\n",
        "    trigger_inputs=trigger_inputs,\n",
        "    trigger_targets=trigger_targets,\n",
        "    epoch_max=5\n",
        ")"
      ],
      "metadata": {
        "id": "QzNDAS9etVAj",
        "outputId": "23c5b174-7969-40d3-b742-ef49bb60c8a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "QzNDAS9etVAj",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Trigger Loss: 2.4723\n",
            "Epoch 1 - Trigger Loss: 2.4471\n",
            "Epoch 2 - Trigger Loss: 2.4433\n",
            "Epoch 3 - Trigger Loss: 2.4269\n",
            "Epoch 4 - Trigger Loss: 2.4330\n",
            "‚úÖ Accuracy on test set: 0.6685\n",
            "üîê Accuracy on trigger set: 0.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}