{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ubaldinho/Hello_World/blob/main/PW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "b303a462f93006ad"
      },
      "cell_type": "markdown",
      "source": [
        "# Practical Work : Secure Federated Learning"
      ],
      "id": "b303a462f93006ad"
    },
    {
      "metadata": {
        "id": "69f1b9298f08bcd4"
      },
      "cell_type": "markdown",
      "source": [
        "Federated Learning (FL) is a machine learning framework that enables $K \\in \\mathbb{N}^*$ participants to collaboratively train a model $M^G$ across $R$ rounds of exchange while maintaining the privacy of their data $D^k$. In the client–server model of FL, the server initializes the global model $M^G_0$. At each round $t$, the global model $M^G_t$ is distributed to a subset $S_t \\subseteq {1, \\dots, K}$ consisting of $C \\times K$ randomly selected clients, where $C \\in (0,1]$. Each client $k \\in S_t$ trains the model locally using its private dataset $D^k$ and sends its updated model $M_{t+1}^k$ back to the server. The server then aggregates these updates to construct the new global model $M_{t+1}^G$. This process repeats until $R$ rounds are completed ($t = R$)."
      ],
      "id": "69f1b9298f08bcd4"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tenseal"
      ],
      "metadata": {
        "id": "anClfl8jkUK4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc2b1f4-7cc8-4a7c-ace5-0c6477e242c0"
      },
      "id": "anClfl8jkUK4",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tenseal in /usr/local/lib/python3.12/dist-packages (0.3.16)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "182723512d0fe76a"
      },
      "cell_type": "code",
      "source": [
        "from src.train import train\n",
        "from src.data_splitter import data_splitter\n",
        "from src.metric import accuracy\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from copy import deepcopy\n",
        "from torch import optim\n",
        "\n",
        "import tenseal as ts"
      ],
      "id": "182723512d0fe76a",
      "outputs": [],
      "execution_count": 2
    },
    {
      "metadata": {
        "id": "280ccaf1937447b"
      },
      "cell_type": "markdown",
      "source": [
        "## Understanding the Client-Server Algorithm"
      ],
      "id": "280ccaf1937447b"
    },
    {
      "metadata": {
        "id": "596dec5faec6bfb6"
      },
      "cell_type": "markdown",
      "source": [
        "In this section, we implement the Federated Learning algorithm using FedAvg. We use the CIFAR-10 dataset and the ResNet-18 model. The server model is trained for 44 rounds, after which one round of training is performed on the clients.\n"
      ],
      "id": "596dec5faec6bfb6"
    },
    {
      "metadata": {
        "id": "c3a1f7642f29247a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa3726a-f73f-4f41-f64a-a22938d2ca0b"
      },
      "cell_type": "code",
      "source": [
        "train_loaders, size, test_loader = data_splitter(\"CIFAR10\", 5 ) # TODO (a) : Load the data using the data_splitter function"
      ],
      "id": "c3a1f7642f29247a",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Dataset :  CIFAR10 \n",
            "\n",
            "Size of the train set for each client : 2000\n",
            "Size of the test set : 10000 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "metadata": {
        "id": "93473d1cb9fc2d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed3ff004-d756-436e-d096-8d6099543ce1"
      },
      "cell_type": "code",
      "source": [
        "server_44 = torch.hub.load('pytorch/vision', 'resnet18', weights=None) # Load the model architecture\n",
        "server_44.fc = nn.Linear(server_44.fc.in_features, 10)\n",
        "server_44.load_state_dict(torch.load(\"/content/model_cifar10_fl.pth\", map_location=torch.device('cpu')) ) # TODO (b) : Load the model weights of the server at round 44\n",
        "#server_44.to(\"cuda\")\n",
        "print(\"Server model at round 44 loaded\")"
      ],
      "id": "93473d1cb9fc2d2",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server model at round 44 loaded\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "metadata": {
        "id": "5f7df5b3f1ef146d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07da6edc-5e59-46ea-c02a-226ea875d76b"
      },
      "cell_type": "code",
      "source": [
        "accuracy(server_44, test_loader)"
      ],
      "id": "5f7df5b3f1ef146d",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.594, 1.167)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "execution_count": 5
    },
    {
      "metadata": {
        "id": "919734bca89aac23"
      },
      "cell_type": "markdown",
      "source": [
        "In the next cell, we define five clients by copying the server model."
      ],
      "id": "919734bca89aac23"
    },
    {
      "metadata": {
        "id": "941b4fc1a1bb2cd"
      },
      "cell_type": "code",
      "source": [
        "clients = []\n",
        "for i in range(5):\n",
        "    client = deepcopy(server_44)\n",
        "    clients.append(client)"
      ],
      "id": "941b4fc1a1bb2cd",
      "outputs": [],
      "execution_count": 6
    },
    {
      "metadata": {
        "id": "c27761a07eb86d65"
      },
      "cell_type": "markdown",
      "source": [
        "Before launching the FL algorithm, we need to define the FedAvg function, which is defined as follows:\n",
        "$$ W^G_{t+1} = \\sum_{k \\in S_t} \\frac{n_k}{n} W^k_{t+1} $$\n",
        "where $W^k_{t+1}$ denotes the weights of client $k$ at round $t+1$, $n_k$ is the size of client $k$’s dataset, and $n$ is the total size of the datasets of all clients."
      ],
      "id": "c27761a07eb86d65"
    },
    {
      "metadata": {
        "id": "ab08460d48f187"
      },
      "cell_type": "code",
      "source": [
        "def fed_avg(server, clients):\n",
        "    with torch.no_grad():\n",
        "        server_next = deepcopy(server)\n",
        "        server_dict = server_next.state_dict()\n",
        "        for name_server in server_dict.keys():\n",
        "            server_dict[name_server].zero_()\n",
        "            for client in clients:\n",
        "                if client.state_dict()[name_server].dtype is torch.long:\n",
        "                    weight = (\n",
        "                        1 / len(clients) # todo (d) : Compute the weight of the client's model\n",
        "                    ) * client.state_dict()[name_server].clone().detach()\n",
        "                    weight = weight.long()\n",
        "\n",
        "                else:\n",
        "                    weight = (\n",
        "                       1 / len(clients) # todo (d) : Compute the weight of the client's model\n",
        "                    ) * client.state_dict()[name_server].clone().detach()\n",
        "\n",
        "                server_dict[name_server].add_(weight)\n",
        "    return server_next"
      ],
      "id": "ab08460d48f187",
      "outputs": [],
      "execution_count": 7
    },
    {
      "metadata": {
        "id": "b17301905822262d"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we can launch the FL algorithm by performing one round of training on the clients."
      ],
      "id": "b17301905822262d"
    },
    {
      "metadata": {
        "id": "f8c053d55bbed546",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e838791-0d99-4b5a-e49f-4a31ef127a06"
      },
      "cell_type": "code",
      "source": [
        "def one_round():\n",
        "    clients = []\n",
        "    for i in range(5):\n",
        "        print(\"Local Training on client\", i)\n",
        "        client = deepcopy(server_44)\n",
        "        clients.append(client.train()) # TODO (d) : Train the client's model using the train function and store it in the clients list\n",
        "    server_next = fed_avg(server_44, clients) # TODO (d) : Aggregate the client's model using the FedAvg algorithm\n",
        "    print(accuracy(server_next, test_loader) ) # TODO (e) : Evaluate the accuracy of the server model after aggregation\n",
        "\n",
        "one_round()"
      ],
      "id": "f8c053d55bbed546",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local Training on client 0\n",
            "Local Training on client 1\n",
            "Local Training on client 2\n",
            "Local Training on client 3\n",
            "Local Training on client 4\n",
            "(0.594, 1.167)\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "metadata": {
        "id": "7f01dd10ca70985e"
      },
      "cell_type": "markdown",
      "source": [
        "## Secure Aggregation using TenSEAL"
      ],
      "id": "7f01dd10ca70985e"
    },
    {
      "metadata": {
        "id": "c8726b2196df0a53"
      },
      "cell_type": "markdown",
      "source": [
        "In this section, we implement secure aggregation using the TenSEAL library. We use the CKKS scheme to encrypt the last layer of each client’s model and then aggregate the encrypted layers. Finally, we decrypt the aggregated layer to obtain the final result."
      ],
      "id": "c8726b2196df0a53"
    },
    {
      "metadata": {
        "id": "46b77ee9c5e542cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12197a59-08b8-4e51-c416-fd223ee012a4"
      },
      "cell_type": "code",
      "source": [
        "server_44 = torch.hub.load('pytorch/vision', 'resnet18', weights=None) # Load the model architecture\n",
        "server_44.fc = nn.Linear(server_44.fc.in_features, 10)\n",
        "server_44.load_state_dict(torch.load(\"/content/model_cifar10_fl.pth\", map_location=torch.device('cpu')) ) # TODO (b) : Load the model weights of the server at round 44\n",
        "#server_44.to(\"cuda\")\n",
        "print(\"Server model at round 44 loaded\")"
      ],
      "id": "46b77ee9c5e542cf",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server model at round 44 loaded\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "metadata": {
        "id": "55748fafe9b20397"
      },
      "cell_type": "markdown",
      "source": [
        "First, we need to define the encryption context using the CKKS scheme."
      ],
      "id": "55748fafe9b20397"
    },
    {
      "metadata": {
        "id": "1575463ec066a2d8"
      },
      "cell_type": "code",
      "source": [
        "ctx = ts.context(ts.SCHEME_TYPE.CKKS, 8192, coeff_mod_bit_sizes=[60, 40, 40, 60])\n",
        "ctx.global_scale = pow(2, 40)\n",
        "ctx.generate_galois_keys()"
      ],
      "id": "1575463ec066a2d8",
      "outputs": [],
      "execution_count": 10
    },
    {
      "metadata": {
        "id": "7975a4383344c517"
      },
      "cell_type": "markdown",
      "source": [
        "Then, we define the function that encrypts the last layer of each client’s model."
      ],
      "id": "7975a4383344c517"
    },
    {
      "metadata": {
        "id": "c88b64d6346a0f8b"
      },
      "cell_type": "code",
      "source": [
        "def encrypt_last_layer(clients, ctx):\n",
        "    encrypted_last_layers = []\n",
        "    for i in range(5):\n",
        "        encrypted_last_layers.append(\n",
        "            ts.ckks_tensor(ctx, clients[i].fc.bias.cpu().detach().numpy()) # TODO (f) : Encrypt the last layer of the client's model\n",
        "        )\n",
        "        # TODO (f) : Encrypt the last layer of the client's model\n",
        "    return encrypted_last_layers"
      ],
      "id": "c88b64d6346a0f8b",
      "outputs": [],
      "execution_count": 11
    },
    {
      "metadata": {
        "id": "f6ea4b568751d06a"
      },
      "cell_type": "code",
      "source": [
        "encrypted_last_layers = encrypt_last_layer(clients, ctx)"
      ],
      "id": "f6ea4b568751d06a",
      "outputs": [],
      "execution_count": 12
    },
    {
      "metadata": {
        "id": "7f2078a84c303edd"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we can aggregate the encrypted last layers and decrypt the aggregated layer to obtain the final result."
      ],
      "id": "7f2078a84c303edd"
    },
    {
      "metadata": {
        "id": "a9a8077ab2a53e7b"
      },
      "cell_type": "code",
      "source": [
        "cli_coeff = 1 / len(clients)\n",
        "aggregated_encrypted_last_layers =  cli_coeff * encrypted_last_layers[0]\n",
        "for i in range(1, 5):\n",
        "    aggregated_encrypted_last_layers += cli_coeff * encrypted_last_layers[i]\n",
        "# TODO (g) : Aggregate the encrypted last layers"
      ],
      "id": "a9a8077ab2a53e7b",
      "outputs": [],
      "execution_count": 13
    },
    {
      "metadata": {
        "id": "d2e68c9104b9f5c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e9ba690-4fc5-4e31-fdfc-d8a5c6bfb0bd"
      },
      "cell_type": "code",
      "source": [
        "result = aggregated_encrypted_last_layers.decrypt().tolist()\n",
        "print(result)\n",
        "# TODO (h) : Decrypt the aggregated last layer and print the result"
      ],
      "id": "d2e68c9104b9f5c9",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.12019600956226988, 0.05193927470126821, 0.1844228005301616, 0.35058398792729095, 0.08478416257873579, -0.16556600546367298, -0.3123740961464586, 0.18795419684798487, -0.07198966570366853, -0.001764628803325899]\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "metadata": {
        "id": "8a9015f73807938b"
      },
      "cell_type": "markdown",
      "source": [
        "You can compare the result with the aggregation of the clients’ models without encryption:"
      ],
      "id": "8a9015f73807938b"
    },
    {
      "metadata": {
        "id": "c78a58a0f6109788",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee1b3d3-f68b-49e6-ef5e-0a30e54b4bd3"
      },
      "cell_type": "code",
      "source": [
        "aggregated_last_layer = 0\n",
        "for i in range(5):\n",
        "    aggregated_last_layer += (1/5) * clients[i].fc.bias.cpu().detach()\n",
        "print(aggregated_last_layer)"
      ],
      "id": "c78a58a0f6109788",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.1202,  0.0519,  0.1844,  0.3506,  0.0848, -0.1656, -0.3124,  0.1880,\n",
            "        -0.0720, -0.0018])\n"
          ]
        }
      ],
      "execution_count": 15
    },
    {
      "metadata": {
        "id": "efd05af781ea40c7"
      },
      "cell_type": "markdown",
      "source": [
        "## Byzantine Attack"
      ],
      "id": "efd05af781ea40c7"
    },
    {
      "metadata": {
        "id": "8f0623eb66f94fa8"
      },
      "cell_type": "markdown",
      "source": [
        "In this section, we implement various Byzantine attacks that aim to compromise the federated learning process by sending malicious updates to the server."
      ],
      "id": "8f0623eb66f94fa8"
    },
    {
      "metadata": {
        "id": "9bca58be356b4ab4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce198997-daeb-46fd-a1b3-a1287e185219"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server model at round 44 loaded\n"
          ]
        }
      ],
      "execution_count": 16,
      "source": [
        "server_44 = torch.hub.load('pytorch/vision', 'resnet18', weights=None) # Load the model architecture\n",
        "server_44.fc = nn.Linear(server_44.fc.in_features, 10)\n",
        "server_44.load_state_dict(torch.load(\"/content/model_cifar10_fl.pth\", map_location=torch.device('cpu')) ) # TODO (b) : Load the model weights of the server at round 44\n",
        "#server_44.to(\"cuda\")\n",
        "print(\"Server model at round 44 loaded\")"
      ],
      "id": "9bca58be356b4ab4"
    },
    {
      "metadata": {
        "id": "a5856b23ef0c5b08"
      },
      "cell_type": "markdown",
      "source": [
        "The next cell contains all the Byzantine attacks that we implement.  \n",
        "The Byzantine attacks are defined as follows:\n",
        "\n",
        "- **Lazy Attack:** A client sends arbitrary values (e.g., random or malformed updates).  \n",
        "- **Same Attack:** A client sends identical values for all parameters.  \n",
        "- **Sign Attack:** A client multiplies all weights by a scalar $\\alpha$ (i.e., scales the model).  \n",
        "- **Noise Attack:** A client adds random noise to the weights (sampled from a chosen distribution, e.g., $\\mathcal{N}(0,\\sigma^2)$).  \n",
        "\n",
        "Finally, we define the filter (defense) that will be used to detect and"
      ],
      "id": "a5856b23ef0c5b08"
    },
    {
      "metadata": {
        "id": "90c32782665d449a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 17,
      "source": [
        "def BA_Lazy(server):\n",
        "    with torch.no_grad():\n",
        "        server_next = deepcopy(server)\n",
        "        server_dict = server_next.state_dict()\n",
        "        for name_server in server_dict.keys():\n",
        "            server_dict[name_server].zero_() # TODO (i) : Fill the server's model with a what you want\n",
        "    return server_next\n",
        "\n",
        "# Byzantine Attack\n",
        "def BA_Same(server):\n",
        "    with torch.no_grad():\n",
        "        server_next = deepcopy(server)\n",
        "        alpha = 100\n",
        "        server_dict = server_next.state_dict()\n",
        "        for name_server in server_dict.keys():\n",
        "            server_dict[name_server].fill_(alpha) # TODO (j) : Fill the server's model with the same value\n",
        "    return server_next\n",
        "\n",
        "def BA_Sign(server):\n",
        "    with torch.no_grad():\n",
        "        server_next = deepcopy(server)\n",
        "        server_dict = server_next.state_dict()\n",
        "        beta = 1000\n",
        "        for name_server in server_dict.keys():\n",
        "            server_dict[name_server].copy_(- server.state_dict()[name_server] * beta) # TODO (j) : Multiply all the weights by a value alpha\n",
        "    return server_next\n",
        "\n",
        "def BA_Noise(server):\n",
        "    with torch.no_grad():\n",
        "        server_next = deepcopy(server)\n",
        "        server_dict = server_next.state_dict()\n",
        "        mu, sigma = 0, 0.1 # TODO (j) : Define the mean and standard deviation of the noise\n",
        "        for name_server in server_dict.keys():\n",
        "            if server_dict[name_server].dtype is not torch.long:\n",
        "                noise = torch.rand_like(server.state_dict()[name_server]) * sigma + mu # TODO (j) : Add some noise to the weights\n",
        "                server_dict[name_server].add_(noise)\n",
        "                #server_dict[name_server].copy_(server.state_dict()[name_server] + noise)\n",
        "    return server_next\n",
        "\n",
        "def filter(client):\n",
        "    \"\"\"\n",
        "    Filtre pour détecter les clients Byzantine\n",
        "    threshold: seuil pour détecter les valeurs identiques\n",
        "    noise_threshold: seuil pour détecter le bruit excessif\n",
        "    \"\"\"\n",
        "\n",
        "    threshold, noise_threshold = 0.1, 0.05\n",
        "    with torch.no_grad():\n",
        "        client_dict = client.state_dict()\n",
        "\n",
        "        # Vérification Same Value Attack\n",
        "        for name, param in client_dict.items():\n",
        "            if param.numel() > 1:  # Éviter les scalaires\n",
        "                unique_values = torch.unique(param)\n",
        "                if len(unique_values) == 1:  # Toutes les valeurs identiques\n",
        "                    print(f\"⚠️ Same Value Attack détecté dans {name}\")\n",
        "                    return False\n",
        "        print(\"✅ Pas de Same Value Attack détecté\")\n",
        "        # Vérification Lazy Attack (tous zéros)\n",
        "        total_norm = 0\n",
        "        for param in client_dict.values():\n",
        "            param = param.float()\n",
        "            total_norm += param.norm().item()\n",
        "\n",
        "        if total_norm < threshold:  # Norme trop petite = modèle trop proche de zéro\n",
        "            print(f\"⚠️ Lazy Attack détecté (norme totale: {total_norm:.6f})\")\n",
        "            return False\n",
        "        print(\"✅ Pas de Lazy Attack détecté\")\n",
        "\n",
        "\n",
        "    return True\n"
      ],
      "id": "90c32782665d449a"
    },
    {
      "metadata": {
        "id": "df42e298985fd72e"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 18,
      "source": [
        "def one_round_attack(server):\n",
        "    # Clients Side\n",
        "    clients = []\n",
        "    for i in range(5):\n",
        "        if i == 4:\n",
        "            print(\"Malicious Client\", i)\n",
        "            client = BA_Lazy(server) # TODO (k) : Apply the Byzantine Attack on the server\n",
        "        else:\n",
        "            print(\"Local Training on client\", i)\n",
        "            client = deepcopy(server)\n",
        "            train(client, train_loaders[i], test_loader, 1)\n",
        "        clients.append(client)\n",
        "\n",
        "    # Server Side\n",
        "    for i in range(5):\n",
        "        print(\"Client\", i)\n",
        "        clients[filter(clients[i])] # TODO (l) : Filter the client's model\n",
        "    server_next = fed_avg(server_44, clients)\n",
        "    print(\"Server Accuracy at round 45 \",accuracy(server_next, test_loader))"
      ],
      "id": "df42e298985fd72e"
    },
    {
      "metadata": {
        "id": "ba10ea5fa2cf91fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a0594dc-e20a-414a-d4dd-374d77ab695b"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local Training on client 0\n",
            "Epoch : 0\n",
            "(0.671, 1.115)\n",
            "Local Training on client 1\n",
            "Epoch : 0\n",
            "(0.67, 1.019)\n",
            "Local Training on client 2\n",
            "Epoch : 0\n",
            "(0.667, 1.182)\n",
            "Local Training on client 3\n",
            "Epoch : 0\n",
            "(0.669, 1.127)\n",
            "Malicious Client 4\n",
            "Client 0\n",
            "✅ Pas de Same Value Attack détecté\n",
            "✅ Pas de Lazy Attack détecté\n",
            "Client 1\n",
            "✅ Pas de Same Value Attack détecté\n",
            "✅ Pas de Lazy Attack détecté\n",
            "Client 2\n",
            "✅ Pas de Same Value Attack détecté\n",
            "✅ Pas de Lazy Attack détecté\n",
            "Client 3\n",
            "✅ Pas de Same Value Attack détecté\n",
            "✅ Pas de Lazy Attack détecté\n",
            "Client 4\n",
            "⚠️ Same Value Attack détecté dans conv1.weight\n",
            "Server Accuracy at round 45  (0.329, 1.907)\n"
          ]
        }
      ],
      "execution_count": 19,
      "source": [
        "one_round_attack(server_44)"
      ],
      "id": "ba10ea5fa2cf91fb"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#HOMEWORK 1\n",
        "def analyze_client_deviation(clients, i, last_layer_only=True):\n",
        "    \"\"\"\n",
        "    Analyse les écarts des poids et biais du client i par rapport aux autres clients.\n",
        "    Détecte les anomalies de type :\n",
        "    - Différence absolue trop élevée\n",
        "    - Ratio absolu trop élevé (indiquant une mise à l'échelle ou inversion)\n",
        "\n",
        "    Args:\n",
        "        clients (list): Liste de modèles clients (instances de nn.Module)\n",
        "        i (int): Index du client à analyser\n",
        "        last_layer_only (bool): Si True, analyse uniquement la dernière couche (nom contenant 'fc') ce qui est le mode le moins couteux mais tout aussi efficace\n",
        "\n",
        "    Returns:\n",
        "        bool: True si le client est conforme, False si des anomalies sont détectées\n",
        "    \"\"\"\n",
        "\n",
        "    # Seuils de détection\n",
        "    #DIFF_THRESHOLD = 0.1\n",
        "    RATIO_THRESHOLD = 2.0\n",
        "\n",
        "    target_dict = clients[i].state_dict()\n",
        "    other_dicts = [clients[j].state_dict() for j in range(len(clients)) if j != i]\n",
        "\n",
        "    suspicious_params = []\n",
        "\n",
        "    for name, param in target_dict.items():\n",
        "        if last_layer_only and 'fc' not in name:\n",
        "            continue  # ignorer les couches sauf la dernière\n",
        "\n",
        "        # Empiler les paramètres des autres clients\n",
        "        others_tensor = torch.stack([other[name] for other in other_dicts])\n",
        "        mean_tensor = torch.mean(others_tensor, dim=0)\n",
        "\n",
        "        # Calcul des écarts\n",
        "        abs_rel_ratio = torch.abs((param - mean_tensor) / (mean_tensor + 1e-8))  # éviter division par zéro\n",
        "\n",
        "        # Vérification des seuils\n",
        "        #if torch.any(abs_diff > DIFF_THRESHOLD):\n",
        "        #    suspicious_params.append((name, 'diff', abs_diff.max().item()))\n",
        "        if torch.any(abs_rel_ratio > RATIO_THRESHOLD):\n",
        "            suspicious_params.append((name, 'ratio', abs_rel_ratio.max().item()))\n",
        "\n",
        "    # Affichage des résultats\n",
        "    if suspicious_params:\n",
        "        print(f\"⚠️ Client {i} présente des anomalies :\")\n",
        "        for name, typ, val in suspicious_params:\n",
        "            print(f\" - {name} dépasse le seuil ({typ} = {val:.4f})\")\n",
        "        return False\n",
        "    else:\n",
        "        print(f\"✅ Client {i} est conforme aux seuils définis.\")\n",
        "        return True"
      ],
      "metadata": {
        "id": "mutGc48jCx0r"
      },
      "id": "mutGc48jCx0r",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Seuils pour la détection de bruit\n",
        "STD_THRESHOLD = 0.1\n",
        "VAR_THRESHOLD = 0.1\n",
        "DISTANCE_THRESHOLD = 0.5\n",
        "COSINE_THRESHOLD = 0.4\n",
        "#HOMEWORK\n",
        "def detect_noise_attack(clients, i, last_layer_only=True):\n",
        "    \"\"\"\n",
        "    Détecte une attaque Byzantine de type bruit (Additive Noise Attack) en comparant\n",
        "    les mises à jour du client i avec celles des autres clients.\n",
        "\n",
        "    Métriques utilisées :\n",
        "    - Écart-type et variance des différences\n",
        "    - Distance euclidienne\n",
        "    - Similarité cosinus\n",
        "\n",
        "    Args:\n",
        "        clients (list): Liste de modèles clients (instances de nn.Module)\n",
        "        i (int): Index du client à analyser\n",
        "        last_layer_only (bool): Si True, analyse uniquement la dernière couche (nom contenant 'fc')\n",
        "\n",
        "    Returns:\n",
        "        bool: True si le client est conforme, False si bruit excessif détecté\n",
        "    \"\"\"\n",
        "    target_dict = clients[i].state_dict()\n",
        "    other_dicts = [clients[j].fc.bias for j in range(len(clients)) if j != i]\n",
        "\n",
        "    noisy_params = []\n",
        "\n",
        "    for name in target_dict.keys():\n",
        "        if last_layer_only and 'fc' not in name:\n",
        "            continue\n",
        "        param = clients[i].fc.bias\n",
        "\n",
        "        # Empiler les paramètres des autres clients\n",
        "        others_tensor = torch.stack([other for other in other_dicts])\n",
        "        mean_tensor = torch.mean(others_tensor, dim=0)\n",
        "\n",
        "        delta = param - mean_tensor\n",
        "\n",
        "        # Métriques statistiques\n",
        "        std_dev = torch.std(delta)\n",
        "        variance = torch.var(delta)\n",
        "        mean_val = torch.mean(delta) + 1e-8\n",
        "\n",
        "        # Distance euclidienne\n",
        "        euclidean_dist = torch.norm(delta)\n",
        "\n",
        "        # Similarité cosinus\n",
        "        cosine_sim = F.cosine_similarity(param.flatten(), mean_tensor.flatten(), dim=0)\n",
        "\n",
        "        if (\n",
        "            std_dev / mean_val > STD_THRESHOLD or\n",
        "            sqrt(variance) / mean_val > VAR_THRESHOLD or\n",
        "            euclidean_dist / mean_val > DISTANCE_THRESHOLD or\n",
        "            cosine_sim < COSINE_THRESHOLD\n",
        "        ):\n",
        "            noisy_params.append((name, std_dev.item(), variance.item(), euclidean_dist.item(), cosine_sim.item()))\n",
        "\n",
        "    if noisy_params:\n",
        "        print(f\"⚠️ Client {i} présente des signes de Noise Attack :\")\n",
        "        for name, std, var, dist, cos in noisy_params:\n",
        "            print(f\" - {name} : std={std:.4f}, var={var:.4f}, dist={dist:.4f}, cos_sim={cos:.4f}\")\n",
        "        return False\n",
        "    else:\n",
        "        print(f\"✅ Client {i} ne présente pas de bruit excessif.\")\n",
        "        return True"
      ],
      "metadata": {
        "id": "XCE5VprWI7t-"
      },
      "id": "XCE5VprWI7t-",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from math import sqrt\n",
        "# HOMEWORK\n",
        "def filter_all(clients, i):\n",
        "    \"\"\"\n",
        "    Filtre pour détecter les clients Byzantine\n",
        "    \"\"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        client = clients[i]\n",
        "        client_dict = client.state_dict()\n",
        "\n",
        "        # Vérification Sign Attack (valeurs extrêmes)\n",
        "        analyze_client_deviation(clients, i)\n",
        "\n",
        "        #verfification de Noisy Attack\n",
        "        detect_noise_attack(clients, i)\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "Ktl72KTxpJGy"
      },
      "id": "Ktl72KTxpJGy",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_round_attack_ext(server, attack_fn, filter_fn = filter_all):\n",
        "    \"\"\"\n",
        "    Exécute un round d'entraînement fédéré avec une attaque Byzantine sur un client.\n",
        "\n",
        "    Args:\n",
        "        server (nn.Module): Modèle global du serveur.\n",
        "        attack_fn (function): Fonction d'attaque Byzantine à appliquer sur le client malveillant.\n",
        "        filter_fn (function): Fonction de filtrage pour détecter les clients suspects.\n",
        "    \"\"\"\n",
        "    clients = []\n",
        "\n",
        "    # Phase client\n",
        "    for i in range(5):\n",
        "        print(f\"Client {i}\")\n",
        "        client = deepcopy(server)\n",
        "\n",
        "        if i == 4:\n",
        "            print(\"⚠️ Malicious Client\", i)\n",
        "            client = attack_fn(client)  # Appliquer l'attaque\n",
        "        else:\n",
        "            print(\"✅ Local Training on client\", i)\n",
        "            train(client, train_loaders[i], test_loader, 1)\n",
        "\n",
        "        clients.append(client)\n",
        "\n",
        "    # Phase serveur : filtrage\n",
        "    filtered_clients = []\n",
        "    for i in range(5):\n",
        "        print(f\"🔍 Analyse du client {i}\")\n",
        "        if filter_fn(clients, i):\n",
        "            filtered_clients.append(clients[i])\n",
        "        else:\n",
        "            print(f\"❌ Client {i} exclu pour comportement suspect\")\n",
        "\n",
        "    # Agrégation\n",
        "    server_next = fed_avg(server, filtered_clients)\n",
        "    print(\"📊 Server Accuracy at round 45:\", accuracy(server_next, test_loader))"
      ],
      "metadata": {
        "id": "Pb7BtDAzLI3f"
      },
      "id": "Pb7BtDAzLI3f",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_round_attack_ext(server_44, BA_Sign)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKwr37MWLlXp",
        "outputId": "c490760a-bdcc-466a-f3ec-4e9af09d80c8"
      },
      "id": "uKwr37MWLlXp",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 0\n",
            "✅ Local Training on client 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.666, 1.104)\n",
            "Client 1\n",
            "✅ Local Training on client 1\n",
            "Epoch : 0\n",
            "(0.664, 1.145)\n",
            "Client 2\n",
            "✅ Local Training on client 2\n",
            "Epoch : 0\n",
            "(0.663, 1.247)\n",
            "Client 3\n",
            "✅ Local Training on client 3\n",
            "Epoch : 0\n",
            "(0.668, 1.5)\n",
            "Client 4\n",
            "⚠️ Malicious Client 4\n",
            "🔍 Analyse du client 0\n",
            "✅ Client 0 est conforme aux seuils définis.\n",
            "⚠️ Client 0 présente des signes de Noise Attack :\n",
            " - fc.weight : std=48.8703, var=2388.3032, dist=147.3621, cos_sim=-1.0000\n",
            " - fc.bias : std=48.8703, var=2388.3032, dist=147.3621, cos_sim=-1.0000\n",
            "🔍 Analyse du client 1\n",
            "✅ Client 1 est conforme aux seuils définis.\n",
            "⚠️ Client 1 présente des signes de Noise Attack :\n",
            " - fc.weight : std=48.8702, var=2388.2971, dist=147.3619, cos_sim=-1.0000\n",
            " - fc.bias : std=48.8702, var=2388.2971, dist=147.3619, cos_sim=-1.0000\n",
            "🔍 Analyse du client 2\n",
            "✅ Client 2 est conforme aux seuils définis.\n",
            "⚠️ Client 2 présente des signes de Noise Attack :\n",
            " - fc.weight : std=48.8704, var=2388.3201, dist=147.3626, cos_sim=-1.0000\n",
            " - fc.bias : std=48.8704, var=2388.3201, dist=147.3626, cos_sim=-1.0000\n",
            "🔍 Analyse du client 3\n",
            "✅ Client 3 est conforme aux seuils définis.\n",
            "⚠️ Client 3 présente des signes de Noise Attack :\n",
            " - fc.weight : std=48.8708, var=2388.3540, dist=147.3636, cos_sim=-1.0000\n",
            " - fc.bias : std=48.8708, var=2388.3540, dist=147.3636, cos_sim=-1.0000\n",
            "🔍 Analyse du client 4\n",
            "⚠️ Client 4 présente des anomalies :\n",
            " - fc.weight dépasse le seuil (ratio = 122265.6562)\n",
            " - fc.bias dépasse le seuil (ratio = 1030.5281)\n",
            "⚠️ Client 4 présente des signes de Noise Attack :\n",
            " - fc.weight : std=195.4817, var=38213.0977, dist=589.4502, cos_sim=-1.0000\n",
            " - fc.bias : std=195.4817, var=38213.0977, dist=589.4502, cos_sim=-1.0000\n",
            "📊 Server Accuracy at round 45: (0.1, nan)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_round_attack_ext(server_44, BA_Noise)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWJbNK_RLdmV",
        "outputId": "a9597da6-c54a-4776-b0a2-59ebcb34a501"
      },
      "id": "JWJbNK_RLdmV",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 0\n",
            "✅ Local Training on client 0\n",
            "Epoch : 0\n",
            "(0.662, 1.351)\n",
            "Client 1\n",
            "✅ Local Training on client 1\n",
            "Epoch : 0\n",
            "(0.66, 1.226)\n",
            "Client 2\n",
            "✅ Local Training on client 2\n",
            "Epoch : 0\n",
            "(0.666, 1.171)\n",
            "Client 3\n",
            "✅ Local Training on client 3\n",
            "Epoch : 0\n",
            "(0.669, 1.235)\n",
            "Client 4\n",
            "⚠️ Malicious Client 4\n",
            "🔍 Analyse du client 0\n",
            "⚠️ Client 0 présente des anomalies :\n",
            " - fc.weight dépasse le seuil (ratio = 3153.5251)\n",
            "✅ Client 0 ne présente pas de bruit excessif.\n",
            "🔍 Analyse du client 1\n",
            "⚠️ Client 1 présente des anomalies :\n",
            " - fc.weight dépasse le seuil (ratio = 13001.1006)\n",
            "✅ Client 1 ne présente pas de bruit excessif.\n",
            "🔍 Analyse du client 2\n",
            "⚠️ Client 2 présente des anomalies :\n",
            " - fc.weight dépasse le seuil (ratio = 878.4620)\n",
            "✅ Client 2 ne présente pas de bruit excessif.\n",
            "🔍 Analyse du client 3\n",
            "⚠️ Client 3 présente des anomalies :\n",
            " - fc.weight dépasse le seuil (ratio = 9969.5381)\n",
            "✅ Client 3 ne présente pas de bruit excessif.\n",
            "🔍 Analyse du client 4\n",
            "⚠️ Client 4 présente des anomalies :\n",
            " - fc.weight dépasse le seuil (ratio = 4533.8330)\n",
            " - fc.bias dépasse le seuil (ratio = 23.7995)\n",
            "⚠️ Client 4 présente des signes de Noise Attack :\n",
            " - fc.weight : std=0.0239, var=0.0006, dist=0.2312, cos_sim=0.9354\n",
            " - fc.bias : std=0.0239, var=0.0006, dist=0.2312, cos_sim=0.9354\n",
            "📊 Server Accuracy at round 45: (0.281, 3.465)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "4046c70f5e0cdd8c"
      },
      "cell_type": "markdown",
      "source": [
        "## Protect the Model's IP using Watermarking"
      ],
      "id": "4046c70f5e0cdd8c"
    },
    {
      "metadata": {
        "id": "65cb6442c8d6f2a8"
      },
      "cell_type": "markdown",
      "source": [
        "In this section, we implement the model watermarking technique defined by Uchida et al. To simplify the implementation, we assume that the model is watermarked by a single client."
      ],
      "id": "65cb6442c8d6f2a8"
    },
    {
      "metadata": {
        "id": "a65821e4f7128c27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea8cb600-5bcf-4085-9760-5ebff74ed68d"
      },
      "cell_type": "code",
      "source": [
        "# Model Watermarking\n",
        "server_44 = torch.hub.load('pytorch/vision', 'resnet18', weights=None)  # Load the model architecture\n",
        "server_44.fc = nn.Linear(server_44.fc.in_features, 10)\n",
        "server_44.load_state_dict(torch.load(\"/content/model_cifar10_fl.pth\", map_location=torch.device('cpu')) ) # TODO (b) : Load the model weights of the server at round 44\n",
        "#server_44.to(\"cuda\")\n",
        "print(\"Server model at round 44 loaded\")"
      ],
      "id": "a65821e4f7128c27",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server model at round 44 loaded\n"
          ]
        }
      ],
      "execution_count": 34
    },
    {
      "metadata": {
        "id": "9fced481d37982b4"
      },
      "cell_type": "markdown",
      "source": [
        "Before watermarking the model, we examine the limitation of a common method used to evaluate whether two models are identical in open-source platforms. In the next cells, we compute the hash of the last layer of the server model."
      ],
      "id": "9fced481d37982b4"
    },
    {
      "metadata": {
        "id": "220bafaf75022f37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc6c13a-a5e9-4942-ac66-a4cb720e90a5"
      },
      "cell_type": "code",
      "source": [
        "server_44.fc.bias"
      ],
      "id": "220bafaf75022f37",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([-0.1202,  0.0519,  0.1844,  0.3506,  0.0848, -0.1656, -0.3124,  0.1880,\n",
              "        -0.0720, -0.0018], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "execution_count": 35
    },
    {
      "metadata": {
        "id": "699101bffee40a3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2288c26e-8a3d-41aa-961c-6c6692d701eb"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.594, 1.167)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "execution_count": 36,
      "source": [
        "accuracy(server_44, test_loader)"
      ],
      "id": "699101bffee40a3a"
    },
    {
      "metadata": {
        "id": "babf7ad095202ba3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8643dca-6fdd-4630-c345-6cc05ccc68b1"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.12019599229097366 0.05193926766514778 0.18442277610301971 0.3505839407444 0.08478415012359619 -0.16556598246097565 -0.3123740553855896 0.18795417249202728 -0.07198965549468994 -0.0017646284541115165 \n",
            "Hash of the layer : 5858509149233719191\n"
          ]
        }
      ],
      "execution_count": 37,
      "source": [
        "tensor_to_str = ''.join(str(x.item())+\" \" for x in server_44.fc.bias)\n",
        "print(tensor_to_str)\n",
        "print(\"Hash of the layer :\", hash(tensor_to_str))"
      ],
      "id": "babf7ad095202ba3"
    },
    {
      "metadata": {
        "id": "6a3bec32ed2c3d45"
      },
      "cell_type": "markdown",
      "source": [
        "Let’s add a small perturbation to the last layer of the server model and then compute the hash again."
      ],
      "id": "6a3bec32ed2c3d45"
    },
    {
      "metadata": {
        "id": "dda0cd69c92de09a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 38,
      "source": [
        "with torch.no_grad():\n",
        "    server_44.fc.bias.add_(1e-3)"
      ],
      "id": "dda0cd69c92de09a"
    },
    {
      "metadata": {
        "id": "207bd7316dd57e05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97211545-f5be-4e3a-a16f-62d1f7adfeec"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.594, 1.167)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "execution_count": 39,
      "source": [
        "accuracy(server_44, test_loader)"
      ],
      "id": "207bd7316dd57e05"
    },
    {
      "metadata": {
        "id": "77746b30a1fd5348",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a7791cf-9b34-4865-82df-9fd40d3907e1"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.11919599026441574 0.052939265966415405 0.18542277812957764 0.35158392786979675 0.08578415215015411 -0.16456598043441772 -0.31137406826019287 0.1889541745185852 -0.07098965346813202 -0.0007646284066140652 \n",
            "Hash of the layer : 8435477227401369874\n"
          ]
        }
      ],
      "execution_count": 40,
      "source": [
        "tensor_to_str = ''.join(str(x.item())+\" \" for x in server_44.fc.bias)\n",
        "print(tensor_to_str)\n",
        "print(\"Hash of the layer :\", hash(tensor_to_str))"
      ],
      "id": "77746b30a1fd5348"
    },
    {
      "metadata": {
        "id": "601648462394ef16"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, the hash is different while the accuracy remains the same. This means that an attacker can bypass this simple method of verifying whether two models are identical."
      ],
      "id": "601648462394ef16"
    },
    {
      "metadata": {
        "id": "6bf3c58d18f415cd"
      },
      "cell_type": "markdown",
      "source": [
        "Now let’s implement the watermarking technique proposed by Uchida *et al.*  \n",
        "This technique consists of embedding a secret message in a layer using the following methodology:\n",
        "\n",
        "1. **Secret generation:** Generate a secret key $K$ and a message $b$.  \n",
        "2. **Parameter selection:** Select the parameters `\"fc.weight\"` and compute the mean along the columns to obtain a vector $w$.  \n",
        "3. **Projection:** Project the vector $w$, in which we want to embed $b$, using the secret key $K$ as follows:  \n",
        "   $$\n",
        "   y = K w\n",
        "   $$  \n",
        "4. **Extraction:** Apply the Sigmoid function to obtain the extracted message $b'$:  \n",
        "   $$\n",
        "   b' = \\sigma(y)\n",
        "   $$  \n",
        "5. **Loss computation:** Compute the binary cross-entropy loss between the extracted message $b'$ and the original message $b$:  \n",
        "   $$\n",
        "   L = \\text{BCELoss}(b', b)\n",
        "   $$"
      ],
      "id": "6bf3c58d18f415cd"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Clé secrète et message à encoder\n",
        "secret_key = torch.randn((256, 512))  # X ∈ ℝ^{T×M}\n",
        "message = torch.randint(2, (256,)).float()  # b ∈ {0,1}^T\n",
        "\n",
        "def train_f(model, train_set, test_set, epoch_max):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion_watermark = nn.BCELoss()\n",
        "    alpha = 5e-1  # pondération de la perte watermark\n",
        "\n",
        "    for epoch in range(epoch_max):\n",
        "        accumulate_loss = 0\n",
        "\n",
        "        for inputs, targets in train_set:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            outputs_predicted = model(inputs)\n",
        "            loss_main = criterion(outputs_predicted, targets)\n",
        "\n",
        "            # (n) Extraction du watermark\n",
        "            fc_weights = model.state_dict()['fc.weight']  # shape: [256, 512]\n",
        "            w_mean = torch.mean(fc_weights, dim=0)  # w̄ ∈ ℝ^256\n",
        "\n",
        "            y = torch.sigmoid(secret_key @ w_mean)  # y ∈ ℝ^256\n",
        "\n",
        "            # (o) Calcul de la perte watermark\n",
        "            loss_watermark = criterion_watermark(y, message)\n",
        "\n",
        "            # Perte totale\n",
        "            loss = loss_main + (alpha * loss_watermark)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            accumulate_loss += loss.item()\n",
        "\n",
        "        # (p) Vérification du watermark : Bit Error Rate\n",
        "        with torch.no_grad():\n",
        "            w_mean = torch.mean(model.state_dict()['fc.weight'], dim=0)\n",
        "            y_extracted = torch.sigmoid(secret_key @ w_mean)\n",
        "            b_extracted = (y_extracted >= 0.5).float()\n",
        "            bit_error_rate = torch.mean(torch.abs(b_extracted - message)).item()\n",
        "\n",
        "        print(f\"Epoch : {epoch}\")\n",
        "        print(\"Bit Error Rate :\", bit_error_rate)"
      ],
      "metadata": {
        "id": "g7uiDCB2Q-0a"
      },
      "id": "g7uiDCB2Q-0a",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Génération du secret sur CPU\n",
        "secret_key = torch.randn((256, 512))\n",
        "message = torch.randint(2, (256,)).float()\n",
        "\n",
        "def train_g(model, train_set, test_set, epoch_max):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion_watermark = nn.BCELoss()  # TODO (o) : Binary Cross Entropy pour le watermark\n",
        "    alpha = 5e-1\n",
        "\n",
        "    for epoch in range(epoch_max):\n",
        "        accumulate_loss = 0\n",
        "\n",
        "        for inputs, outputs in train_set:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            outputs_predicted = model(inputs)\n",
        "            loss_main = criterion(outputs_predicted, outputs)\n",
        "\n",
        "            # TODO (n) : Extraction du message depuis le modèle\n",
        "            w = model.fc.weight.mean(dim=0)  # Moyenne des colonnes de fc.weight\n",
        "            y = torch.matmul(secret_key, w)  # Projection avec la clé secrète\n",
        "            extracted_message = torch.sigmoid(y)  # Application sigmoid\n",
        "\n",
        "            # TODO (o) : Calcul de la loss du watermark\n",
        "            loss_watermark = criterion_watermark(extracted_message, message)\n",
        "\n",
        "            loss = loss_main + (alpha * loss_watermark)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            accumulate_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch : {epoch}\")\n",
        "        print(accuracy(model, test_set))\n",
        "\n",
        "        # TODO (p) : Calcul du Bit Error Rate\n",
        "        predicted_bits = (extracted_message > 0.5).float()\n",
        "        bit_error_rate = (predicted_bits != message).float().mean().item()\n",
        "        print(\"Bit Error Rate : \", bit_error_rate)"
      ],
      "metadata": {
        "id": "MxmPGKNe0YrL"
      },
      "id": "MxmPGKNe0YrL",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = deepcopy(server_44)\n",
        "train_f(client, train_loaders[4], test_loader, 1)"
      ],
      "metadata": {
        "id": "1AHQgg-sSlON",
        "outputId": "fc609e4a-0e1d-48dd-e390-14afdb90b487",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1AHQgg-sSlON",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0\n",
            "Bit Error Rate : 0.5078125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = deepcopy(server_44)\n",
        "\n",
        "train_g(client, train_loaders[4], test_loader, 1)"
      ],
      "metadata": {
        "id": "b0k3itIafpT2",
        "outputId": "d869be59-f5c7-48e8-a1d2-8eb718a86ebf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "b0k3itIafpT2",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.664, 0.952)\n",
            "Bit Error Rate :  0.3671875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Étape 2 : Créer une copie pour y intégrer le watermark\n",
        "model_watermarked = deepcopy(server_44)\n",
        "\n",
        "# Étape 3 : Créer le trigger set\n",
        "def create_trigger_set(trigger_label=7, num_samples=100):\n",
        "    \"\"\"\n",
        "    Génère un ensemble d'images avec un motif constant et une étiquette fixe.\n",
        "    Ce set est utilisé pour encoder un watermark dans le comportement du modèle.\n",
        "    \"\"\"\n",
        "    trigger_inputs = torch.ones((num_samples, 3, 32, 32)) * 0.5  # motif constant\n",
        "    trigger_targets = torch.full((num_samples,), trigger_label, dtype=torch.long)\n",
        "    return trigger_inputs, trigger_targets\n",
        "\n",
        "trigger_inputs, trigger_targets = create_trigger_set()\n",
        "\n",
        "# Étape 4 : Entraîner le modèle avec le watermark\n",
        "def train_with_blackbox_watermark(model, train_loader, test_loader, trigger_inputs, trigger_targets, epoch_max):\n",
        "    \"\"\"\n",
        "    Entraîne le modèle à répondre au trigger set tout en conservant ses performances sur le test set.\n",
        "    \"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epoch_max):\n",
        "        model.train()\n",
        "        for inputs, targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Entraînement sur le trigger set\n",
        "        optimizer.zero_grad()\n",
        "        outputs_trigger = model(trigger_inputs)\n",
        "        loss_trigger = criterion(outputs_trigger, trigger_targets)\n",
        "        loss_trigger.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch} - Trigger Loss: {loss_trigger.item():.4f}\")\n",
        "\n",
        "    # Évaluation\n",
        "    def evaluate(loader):\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                outputs = model(inputs)\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                correct += (preds == targets).sum().item()\n",
        "                total += targets.size(0)\n",
        "        return correct / total\n",
        "\n",
        "    acc_test = evaluate(test_loader)\n",
        "    acc_trigger = torch.mean((torch.argmax(model(trigger_inputs), dim=1) == trigger_targets).float()).item()\n",
        "\n",
        "    print(\"✅ Accuracy on test set:\", acc_test)\n",
        "    print(\"🔐 Accuracy on trigger set:\", acc_trigger)\n",
        "\n",
        "# Étape 5 : Appel de la fonction avec des loaders CIFAR-10\n",
        "train_with_blackbox_watermark(\n",
        "    model_watermarked,\n",
        "    train_loader=train_loaders[4],  # loader client\n",
        "    test_loader=test_loader,        # loader global\n",
        "    trigger_inputs=trigger_inputs,\n",
        "    trigger_targets=trigger_targets,\n",
        "    epoch_max=5\n",
        ")"
      ],
      "metadata": {
        "id": "QzNDAS9etVAj",
        "outputId": "23c5b174-7969-40d3-b742-ef49bb60c8a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "QzNDAS9etVAj",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Trigger Loss: 2.4723\n",
            "Epoch 1 - Trigger Loss: 2.4471\n",
            "Epoch 2 - Trigger Loss: 2.4433\n",
            "Epoch 3 - Trigger Loss: 2.4269\n",
            "Epoch 4 - Trigger Loss: 2.4330\n",
            "✅ Accuracy on test set: 0.6685\n",
            "🔐 Accuracy on trigger set: 0.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}