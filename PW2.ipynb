{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ubaldinho/Hello_World/blob/main/PW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "b303a462f93006ad"
      },
      "cell_type": "markdown",
      "source": [
        "# Practical Work : Secure Federated Learning"
      ],
      "id": "b303a462f93006ad"
    },
    {
      "metadata": {
        "id": "69f1b9298f08bcd4"
      },
      "cell_type": "markdown",
      "source": [
        "Federated Learning (FL) is a machine learning framework that enables $K \\in \\mathbb{N}^*$ participants to collaboratively train a model $M^G$ across $R$ rounds of exchange while maintaining the privacy of their data $D^k$. In the client–server model of FL, the server initializes the global model $M^G_0$. At each round $t$, the global model $M^G_t$ is distributed to a subset $S_t \\subseteq {1, \\dots, K}$ consisting of $C \\times K$ randomly selected clients, where $C \\in (0,1]$. Each client $k \\in S_t$ trains the model locally using its private dataset $D^k$ and sends its updated model $M_{t+1}^k$ back to the server. The server then aggregates these updates to construct the new global model $M_{t+1}^G$. This process repeats until $R$ rounds are completed ($t = R$)."
      ],
      "id": "69f1b9298f08bcd4"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tenseal"
      ],
      "metadata": {
        "id": "anClfl8jkUK4",
        "outputId": "3417c440-ada3-4a87-947d-ba8c0e5d80c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "anClfl8jkUK4",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tenseal\n",
            "  Downloading tenseal-0.3.16-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Downloading tenseal-0.3.16-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenseal\n",
            "Successfully installed tenseal-0.3.16\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "182723512d0fe76a"
      },
      "cell_type": "code",
      "source": [
        "from src.train import train\n",
        "from src.data_splitter import data_splitter\n",
        "from src.metric import accuracy\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from copy import deepcopy\n",
        "from torch import optim\n",
        "\n",
        "import tenseal as ts"
      ],
      "id": "182723512d0fe76a",
      "outputs": [],
      "execution_count": 2
    },
    {
      "metadata": {
        "id": "280ccaf1937447b"
      },
      "cell_type": "markdown",
      "source": [
        "## Understanding the Client-Server Algorithm"
      ],
      "id": "280ccaf1937447b"
    },
    {
      "metadata": {
        "id": "596dec5faec6bfb6"
      },
      "cell_type": "markdown",
      "source": [
        "In this section, we implement the Federated Learning algorithm using FedAvg. We use the CIFAR-10 dataset and the ResNet-18 model. The server model is trained for 44 rounds, after which one round of training is performed on the clients.\n"
      ],
      "id": "596dec5faec6bfb6"
    },
    {
      "metadata": {
        "id": "c3a1f7642f29247a",
        "outputId": "a55b2622-7d47-4d1b-ca98-e6083d5aaeb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "train_loaders, size, test_loader = data_splitter(\"CIFAR10\", 5 ) # TODO (a) : Load the data using the data_splitter function"
      ],
      "id": "c3a1f7642f29247a",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Dataset :  CIFAR10 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the train set for each client : 2000\n",
            "Size of the test set : 10000 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "metadata": {
        "id": "93473d1cb9fc2d2",
        "outputId": "d08f38a9-bd8d-4662-f356-11c2fef1e39b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "server_44 = torch.hub.load('pytorch/vision', 'resnet18', weights=None) # Load the model architecture\n",
        "server_44.fc = nn.Linear(server_44.fc.in_features, 10)\n",
        "server_44.load_state_dict(torch.load(\"/content/model_cifar10_fl.pth\") ) # TODO (b) : Load the model weights of the server at round 44\n",
        "server_44.to(\"cuda\")\n",
        "print(\"Server model at round 44 loaded\")"
      ],
      "id": "93473d1cb9fc2d2",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server model at round 44 loaded\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "metadata": {
        "id": "5f7df5b3f1ef146d",
        "outputId": "cb3adb44-008e-4c28-d3b9-b804820c8760",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "accuracy(server_44, test_loader)"
      ],
      "id": "5f7df5b3f1ef146d",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.594, 1.167)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "execution_count": 14
    },
    {
      "metadata": {
        "id": "919734bca89aac23"
      },
      "cell_type": "markdown",
      "source": [
        "In the next cell, we define five clients by copying the server model."
      ],
      "id": "919734bca89aac23"
    },
    {
      "metadata": {
        "id": "941b4fc1a1bb2cd"
      },
      "cell_type": "code",
      "source": [
        "clients = []\n",
        "for i in range(5):\n",
        "    client = deepcopy(server_44)\n",
        "    clients.append(client)"
      ],
      "id": "941b4fc1a1bb2cd",
      "outputs": [],
      "execution_count": 15
    },
    {
      "metadata": {
        "id": "c27761a07eb86d65"
      },
      "cell_type": "markdown",
      "source": [
        "Before launching the FL algorithm, we need to define the FedAvg function, which is defined as follows:\n",
        "$$ W^G_{t+1} = \\sum_{k \\in S_t} \\frac{n_k}{n} W^k_{t+1} $$\n",
        "where $W^k_{t+1}$ denotes the weights of client $k$ at round $t+1$, $n_k$ is the size of client $k$’s dataset, and $n$ is the total size of the datasets of all clients."
      ],
      "id": "c27761a07eb86d65"
    },
    {
      "metadata": {
        "id": "ab08460d48f187"
      },
      "cell_type": "code",
      "source": [
        "def fed_avg(server, clients):\n",
        "    with torch.no_grad():\n",
        "        server_next = deepcopy(server)\n",
        "        server_dict = server_next.state_dict()\n",
        "        for name_server in server_dict.keys():\n",
        "            server_dict[name_server].zero_()\n",
        "            for client in clients:\n",
        "                if client.state_dict()[name_server].dtype is torch.long:\n",
        "                    weight = (\n",
        "                        1 / len(clients) # todo (d) : Compute the weight of the client's model\n",
        "                    ) * client.state_dict()[name_server].clone().detach()\n",
        "                    weight = weight.long()\n",
        "\n",
        "                else:\n",
        "                    weight = (\n",
        "                       1 / len(clients) # todo (d) : Compute the weight of the client's model\n",
        "                    ) * client.state_dict()[name_server].clone().detach()\n",
        "\n",
        "                server_dict[name_server].add_(weight)\n",
        "    return server_next"
      ],
      "id": "ab08460d48f187",
      "outputs": [],
      "execution_count": 16
    },
    {
      "metadata": {
        "id": "b17301905822262d"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we can launch the FL algorithm by performing one round of training on the clients."
      ],
      "id": "b17301905822262d"
    },
    {
      "metadata": {
        "id": "f8c053d55bbed546",
        "outputId": "d39f74ae-d398-42fc-8409-ddb011bd8834",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "def one_round():\n",
        "    clients = []\n",
        "    for i in range(5):\n",
        "        print(\"Local Training on client\", i)\n",
        "        client = deepcopy(server_44)\n",
        "        clients.append(client.train()) # TODO (d) : Train the client's model using the train function and store it in the clients list\n",
        "    server_next = fed_avg(server_44, clients) # TODO (d) : Aggregate the client's model using the FedAvg algorithm\n",
        "    print(accuracy(server_next, test_loader) ) # TODO (e) : Evaluate the accuracy of the server model after aggregation\n",
        "\n",
        "one_round()"
      ],
      "id": "f8c053d55bbed546",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local Training on client 0\n",
            "Local Training on client 1\n",
            "Local Training on client 2\n",
            "Local Training on client 3\n",
            "Local Training on client 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.594, 1.167)\n"
          ]
        }
      ],
      "execution_count": 20
    },
    {
      "metadata": {
        "id": "7f01dd10ca70985e"
      },
      "cell_type": "markdown",
      "source": [
        "## Secure Aggregation using TenSEAL"
      ],
      "id": "7f01dd10ca70985e"
    },
    {
      "metadata": {
        "id": "c8726b2196df0a53"
      },
      "cell_type": "markdown",
      "source": [
        "In this section, we implement secure aggregation using the TenSEAL library. We use the CKKS scheme to encrypt the last layer of each client’s model and then aggregate the encrypted layers. Finally, we decrypt the aggregated layer to obtain the final result."
      ],
      "id": "c8726b2196df0a53"
    },
    {
      "metadata": {
        "id": "46b77ee9c5e542cf",
        "outputId": "895783bd-2e8d-4c92-f063-947d3e80c19b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "server_44 = torch.hub.load('pytorch/vision', 'resnet18', weights=None) # Load the model architecture\n",
        "server_44.fc = nn.Linear(server_44.fc.in_features, 10)\n",
        "server_44.load_state_dict(torch.load(\"/content/model_cifar10_fl.pth\")) # TODO (b) : Load the model weights of the server at round 44\n",
        "server_44.to(\"cuda\")\n",
        "print(\"Server model at round 44 loaded\")"
      ],
      "id": "46b77ee9c5e542cf",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server model at round 44 loaded\n"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "metadata": {
        "id": "55748fafe9b20397"
      },
      "cell_type": "markdown",
      "source": [
        "First, we need to define the encryption context using the CKKS scheme."
      ],
      "id": "55748fafe9b20397"
    },
    {
      "metadata": {
        "id": "1575463ec066a2d8"
      },
      "cell_type": "code",
      "source": [
        "ctx = ts.context(ts.SCHEME_TYPE.CKKS, 8192, coeff_mod_bit_sizes=[60, 40, 40, 60])\n",
        "ctx.global_scale = pow(2, 40)\n",
        "ctx.generate_galois_keys()"
      ],
      "id": "1575463ec066a2d8",
      "outputs": [],
      "execution_count": 21
    },
    {
      "metadata": {
        "id": "7975a4383344c517"
      },
      "cell_type": "markdown",
      "source": [
        "Then, we define the function that encrypts the last layer of each client’s model."
      ],
      "id": "7975a4383344c517"
    },
    {
      "metadata": {
        "id": "c88b64d6346a0f8b"
      },
      "cell_type": "code",
      "source": [
        "def encrypt_last_layer(clients, ctx):\n",
        "    encrypted_last_layers = []\n",
        "    for i in range(5):\n",
        "        encrypted_last_layers.append(\n",
        "            ts.ckks_tensor(ctx, clients[i].fc.bias.cpu().detach().numpy()) # TODO (f) : Encrypt the last layer of the client's model\n",
        "        )\n",
        "        # TODO (f) : Encrypt the last layer of the client's model\n",
        "    return encrypted_last_layers"
      ],
      "id": "c88b64d6346a0f8b",
      "outputs": [],
      "execution_count": 22
    },
    {
      "metadata": {
        "id": "f6ea4b568751d06a"
      },
      "cell_type": "code",
      "source": [
        "encrypted_last_layers = encrypt_last_layer(clients, ctx)"
      ],
      "id": "f6ea4b568751d06a",
      "outputs": [],
      "execution_count": 23
    },
    {
      "metadata": {
        "id": "7f2078a84c303edd"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we can aggregate the encrypted last layers and decrypt the aggregated layer to obtain the final result."
      ],
      "id": "7f2078a84c303edd"
    },
    {
      "metadata": {
        "id": "a9a8077ab2a53e7b"
      },
      "cell_type": "code",
      "source": [
        "cli_coeff = 1 / len(clients)\n",
        "aggregated_encrypted_last_layers =  cli_coeff * encrypted_last_layers[0]\n",
        "for i in range(1, 5):\n",
        "    aggregated_encrypted_last_layers += cli_coeff * encrypted_last_layers[i]\n",
        "# TODO (g) : Aggregate the encrypted last layers"
      ],
      "id": "a9a8077ab2a53e7b",
      "outputs": [],
      "execution_count": 30
    },
    {
      "metadata": {
        "id": "d2e68c9104b9f5c9",
        "outputId": "a3440b7f-071d-4547-e077-df83f354add8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "result = aggregated_encrypted_last_layers.decrypt().tolist()\n",
        "print(result)\n",
        "# TODO (h) : Decrypt the aggregated last layer and print the result"
      ],
      "id": "d2e68c9104b9f5c9",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.12019600696184722, 0.05193927337635204, 0.18442280107928682, 0.3505839870949573, 0.08478416185381495, -0.16556600793842674, -0.31237410052092623, 0.18795420259623194, -0.0719896659901517, -0.0017646292567670052]\n"
          ]
        }
      ],
      "execution_count": 34
    },
    {
      "metadata": {
        "id": "8a9015f73807938b"
      },
      "cell_type": "markdown",
      "source": [
        "You can compare the result with the aggregation of the clients’ models without encryption:"
      ],
      "id": "8a9015f73807938b"
    },
    {
      "metadata": {
        "id": "c78a58a0f6109788",
        "outputId": "ba2e6aee-e270-4d23-e48b-403b1068b8ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "aggregated_last_layer = 0\n",
        "for i in range(5):\n",
        "    aggregated_last_layer += (1/5) * clients[i].fc.bias.cpu().detach()\n",
        "print(aggregated_last_layer)"
      ],
      "id": "c78a58a0f6109788",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.1202,  0.0519,  0.1844,  0.3506,  0.0848, -0.1656, -0.3124,  0.1880,\n",
            "        -0.0720, -0.0018])\n"
          ]
        }
      ],
      "execution_count": 35
    },
    {
      "metadata": {
        "id": "efd05af781ea40c7"
      },
      "cell_type": "markdown",
      "source": [
        "## Byzantine Attack"
      ],
      "id": "efd05af781ea40c7"
    },
    {
      "metadata": {
        "id": "8f0623eb66f94fa8"
      },
      "cell_type": "markdown",
      "source": [
        "In this section, we implement various Byzantine attacks that aim to compromise the federated learning process by sending malicious updates to the server."
      ],
      "id": "8f0623eb66f94fa8"
    },
    {
      "metadata": {
        "id": "9bca58be356b4ab4",
        "outputId": "65493985-a85f-4bea-c8ef-43624dde4381",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server model at round 44 loaded\n"
          ]
        }
      ],
      "execution_count": 36,
      "source": [
        "server_44 = torch.hub.load('pytorch/vision', 'resnet18', weights=None) # Load the model architecture\n",
        "server_44.fc = nn.Linear(server_44.fc.in_features, 10)\n",
        "server_44.load_state_dict(torch.load(\"/content/model_cifar10_fl.pth\")) # TODO (b) : Load the model weights of the server at round 44\n",
        "server_44.to(\"cuda\")\n",
        "print(\"Server model at round 44 loaded\")"
      ],
      "id": "9bca58be356b4ab4"
    },
    {
      "metadata": {
        "id": "a5856b23ef0c5b08"
      },
      "cell_type": "markdown",
      "source": [
        "The next cell contains all the Byzantine attacks that we implement.  \n",
        "The Byzantine attacks are defined as follows:\n",
        "\n",
        "- **Lazy Attack:** A client sends arbitrary values (e.g., random or malformed updates).  \n",
        "- **Same Attack:** A client sends identical values for all parameters.  \n",
        "- **Sign Attack:** A client multiplies all weights by a scalar $\\alpha$ (i.e., scales the model).  \n",
        "- **Noise Attack:** A client adds random noise to the weights (sampled from a chosen distribution, e.g., $\\mathcal{N}(0,\\sigma^2)$).  \n",
        "\n",
        "Finally, we define the filter (defense) that will be used to detect and"
      ],
      "id": "a5856b23ef0c5b08"
    },
    {
      "metadata": {
        "id": "90c32782665d449a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 38,
      "source": [
        "def BA_Lazy(server):\n",
        "    with torch.no_grad():\n",
        "        server_next = deepcopy(server)\n",
        "        server_dict = server_next.state_dict()\n",
        "        for name_server in server_dict.keys():\n",
        "            server_dict[name_server].zero_() # TODO (i) : Fill the server's model with a what you want\n",
        "    return server_next\n",
        "\n",
        "# Byzantine Attack\n",
        "def BA_Same(server):\n",
        "    with torch.no_grad():\n",
        "        server_next = deepcopy(server)\n",
        "        alpha = 100\n",
        "        server_dict = server_next.state_dict()\n",
        "        for name_server in server_dict.keys():\n",
        "            server_dict[name_server].fill_(alpha) # TODO (j) : Fill the server's model with the same value\n",
        "    return server_next\n",
        "\n",
        "def BA_Sign(server):\n",
        "    with torch.no_grad():\n",
        "        server_next = deepcopy(server)\n",
        "        server_dict = server_next.state_dict()\n",
        "        beta = 1000\n",
        "        for name_server in server_dict.keys():\n",
        "            server_dict[name_server].copy_(- server.state_dict()[name_server] * beta) # TODO (j) : Multiply all the weights by a value alpha\n",
        "    return server_next\n",
        "\n",
        "def BA_Noise(server):\n",
        "    with torch.no_grad():\n",
        "        server_next = deepcopy(server)\n",
        "        server_dict = server_next.state_dict()\n",
        "        mu, sigma = 0, 0.1 # TODO (j) : Define the mean and standard deviation of the noise\n",
        "        for name_server in server_dict.keys():\n",
        "            if server_dict[name_server].dtype is not torch.long:\n",
        "                noise = torch.rand_like(server.state_dict()[name_server]) * sigma + mu # TODO (j) : Add some noise to the weights\n",
        "                server_dict[name_server].add_(noise)\n",
        "                #server_dict[name_server].copy_(server.state_dict()[name_server] + noise)\n",
        "    return server_next\n",
        "\n",
        "def filter(client):\n",
        "    \"\"\"\n",
        "    Filtre pour détecter les clients Byzantine\n",
        "    threshold: seuil pour détecter les valeurs identiques\n",
        "    noise_threshold: seuil pour détecter le bruit excessif\n",
        "    \"\"\"\n",
        "\n",
        "    threshold, noise_threshold = 0.1, 0.05\n",
        "    with torch.no_grad():\n",
        "        client_dict = client.state_dict()\n",
        "\n",
        "        # Vérification Same Value Attack\n",
        "        for name, param in client_dict.items():\n",
        "            if param.numel() > 1:  # Éviter les scalaires\n",
        "                unique_values = torch.unique(param)\n",
        "                if len(unique_values) == 1:  # Toutes les valeurs identiques\n",
        "                    print(f\"⚠️ Same Value Attack détecté dans {name}\")\n",
        "                    return False\n",
        "\n",
        "        # Vérification Lazy Attack (tous zéros)\n",
        "        total_norm = 0\n",
        "        for param in client_dict.values():\n",
        "            total_norm += param.norm().item()\n",
        "\n",
        "        if total_norm < threshold:  # Norme trop petite = modèle trop proche de zéro\n",
        "            print(f\"⚠️ Lazy Attack détecté (norme totale: {total_norm:.6f})\")\n",
        "            return False\n",
        "\n",
        "        # Vérification Sign Attack (valeurs extrêmes)\n",
        "        max_abs_value = max(torch.max(torch.abs(param)).item() for param in client_dict.values())\n",
        "        if max_abs_value > 1e3:  # Valeurs extrêmement grandes\n",
        "            print(f\"⚠️ Sign/Scale Attack détecté (valeur max: {max_abs_value:.6f})\")\n",
        "            return False\n",
        "\n",
        "    return True\n"
      ],
      "id": "90c32782665d449a"
    },
    {
      "metadata": {
        "id": "df42e298985fd72e"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 41,
      "source": [
        "def one_round_attack(server):\n",
        "    # Clients Side\n",
        "    clients = []\n",
        "    for i in range(5):\n",
        "        if i == 4:\n",
        "            print(\"Malicious Client\", i)\n",
        "            client = BA_Lazy(server) # TODO (k) : Apply the Byzantine Attack on the server\n",
        "        else:\n",
        "            print(\"Local Training on client\", i)\n",
        "            client = deepcopy(server)\n",
        "            train(client, train_loaders[i], test_loader, 1)\n",
        "        clients.append(client)\n",
        "\n",
        "    # Server Side\n",
        "    for i in range(5):\n",
        "        print(\"Client\", i)\n",
        "        clients[filter(client[i])] # TODO (l) : Filter the client's model\n",
        "    server_next = fedavg(server_44, clients)\n",
        "    print(\"Server Accuracy at round 45 \",accuracy(server_next, test_loader))"
      ],
      "id": "df42e298985fd72e"
    },
    {
      "metadata": {
        "id": "ba10ea5fa2cf91fb",
        "outputId": "70898d09-3af0-4658-c90b-a6a99f1de17a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local Training on client 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0\n",
            "(0.664, 1.5)\n",
            "Local Training on client 1\n",
            "Epoch : 0\n",
            "(0.671, 1.081)\n",
            "Local Training on client 2\n",
            "Epoch : 0\n",
            "(0.672, 1.142)\n",
            "Local Training on client 3\n",
            "Epoch : 0\n",
            "(0.666, 1.097)\n",
            "Malicious Client 4\n",
            "Client 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'ResNet' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3969031504.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mone_round_attack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_44\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2633031223.py\u001b[0m in \u001b[0;36mone_round_attack\u001b[0;34m(server)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Client\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mclients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# TODO (l) : Filter the client's model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mserver_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfedavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_44\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Server Accuracy at round 45 \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'ResNet' object is not subscriptable"
          ]
        }
      ],
      "execution_count": 42,
      "source": [
        "one_round_attack(server_44)"
      ],
      "id": "ba10ea5fa2cf91fb"
    },
    {
      "metadata": {
        "id": "4046c70f5e0cdd8c"
      },
      "cell_type": "markdown",
      "source": [
        "## Protect the Model's IP using Watermarking"
      ],
      "id": "4046c70f5e0cdd8c"
    },
    {
      "metadata": {
        "id": "65cb6442c8d6f2a8"
      },
      "cell_type": "markdown",
      "source": [
        "In this section, we implement the model watermarking technique defined by Uchida et al. To simplify the implementation, we assume that the model is watermarked by a single client."
      ],
      "id": "65cb6442c8d6f2a8"
    },
    {
      "metadata": {
        "id": "a65821e4f7128c27"
      },
      "cell_type": "code",
      "source": [
        "# Model Watermarking\n",
        "server_44 = torch.hub.load('pytorch/vision', 'resnet18', weights=None)  # Load the model architecture\n",
        "server_44.fc = nn.Linear(server_44.fc.in_features, 10)\n",
        "server_44.load_state_dict(torch.load(\"/content/model_cifar10_fl.pth\")) # TODO (b) : Load the model weights of the server at round 44\n",
        "server_44.to(\"cuda\")\n",
        "print(\"Server model at round 44 loaded\")"
      ],
      "id": "a65821e4f7128c27",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "9fced481d37982b4"
      },
      "cell_type": "markdown",
      "source": [
        "Before watermarking the model, we examine the limitation of a common method used to evaluate whether two models are identical in open-source platforms. In the next cells, we compute the hash of the last layer of the server model."
      ],
      "id": "9fced481d37982b4"
    },
    {
      "metadata": {
        "id": "220bafaf75022f37"
      },
      "cell_type": "code",
      "source": [
        "server_44.fc.bias"
      ],
      "id": "220bafaf75022f37",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "699101bffee40a3a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "accuracy(server_44, test_loader)"
      ],
      "id": "699101bffee40a3a"
    },
    {
      "metadata": {
        "id": "babf7ad095202ba3"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "tensor_to_str = ''.join(str(x.item())+\" \" for x in server_44.fc.bias)\n",
        "print(tensor_to_str)\n",
        "print(\"Hash of the layer :\", hash(tensor_to_str))"
      ],
      "id": "babf7ad095202ba3"
    },
    {
      "metadata": {
        "id": "6a3bec32ed2c3d45"
      },
      "cell_type": "markdown",
      "source": [
        "Let’s add a small perturbation to the last layer of the server model and then compute the hash again."
      ],
      "id": "6a3bec32ed2c3d45"
    },
    {
      "metadata": {
        "id": "dda0cd69c92de09a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "with torch.no_grad():\n",
        "    server_44.fc.bias.add_(1e-3)"
      ],
      "id": "dda0cd69c92de09a"
    },
    {
      "metadata": {
        "id": "207bd7316dd57e05"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "accuracy(server_44, test_loader)"
      ],
      "id": "207bd7316dd57e05"
    },
    {
      "metadata": {
        "id": "77746b30a1fd5348"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "tensor_to_str = ''.join(str(x.item())+\" \" for x in server_44.fc.bias)\n",
        "print(tensor_to_str)\n",
        "print(\"Hash of the layer :\", hash(tensor_to_str))"
      ],
      "id": "77746b30a1fd5348"
    },
    {
      "metadata": {
        "id": "601648462394ef16"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, the hash is different while the accuracy remains the same. This means that an attacker can bypass this simple method of verifying whether two models are identical."
      ],
      "id": "601648462394ef16"
    },
    {
      "metadata": {
        "id": "6bf3c58d18f415cd"
      },
      "cell_type": "markdown",
      "source": [
        "Now let’s implement the watermarking technique proposed by Uchida *et al.*  \n",
        "This technique consists of embedding a secret message in a layer using the following methodology:\n",
        "\n",
        "1. **Secret generation:** Generate a secret key $K$ and a message $b$.  \n",
        "2. **Parameter selection:** Select the parameters `\"fc.weight\"` and compute the mean along the columns to obtain a vector $w$.  \n",
        "3. **Projection:** Project the vector $w$, in which we want to embed $b$, using the secret key $K$ as follows:  \n",
        "   $$\n",
        "   y = K w\n",
        "   $$  \n",
        "4. **Extraction:** Apply the Sigmoid function to obtain the extracted message $b'$:  \n",
        "   $$\n",
        "   b' = \\sigma(y)\n",
        "   $$  \n",
        "5. **Loss computation:** Compute the binary cross-entropy loss between the extracted message $b'$ and the original message $b$:  \n",
        "   $$\n",
        "   L = \\text{BCELoss}(b', b)\n",
        "   $$"
      ],
      "id": "6bf3c58d18f415cd"
    },
    {
      "metadata": {
        "id": "649fb8026dbd7a08"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "secret_key = torch.randn((256,512), device=\"cuda\")\n",
        "message = torch.randint(2, (256,), device=\"cuda\").float()\n",
        "def train(model, train_set, test_set, epoch_max):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion_watermark = # TODO (o) : Define the criterion for the watermark\n",
        "    alpha = 5e-1\n",
        "\n",
        "    for epoch in range(epoch_max):\n",
        "        accumulate_loss = 0\n",
        "\n",
        "        for inputs, outputs in train_set:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            inputs = inputs.to(\"cuda\")\n",
        "\n",
        "            outputs = outputs.to(\"cuda\")\n",
        "\n",
        "            outputs_predicted = model(inputs)\n",
        "\n",
        "            loss_main = criterion(outputs_predicted, outputs)\n",
        "\n",
        "            extracted_message = # TODO (n) : Extract the message from the model\n",
        "\n",
        "            loss_watermark  = # TODO (o) : Compute the loss of the watermark\n",
        "\n",
        "            loss = loss_main + (alpha * loss_watermark)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            accumulate_loss += loss.item()\n",
        "        print(f\"Epoch : {epoch}\")\n",
        "        print(accuracy(model, test_set))\n",
        "        print(\"Bit Error Rate : \", # TODO (p) : Compute the Bit Error Rate"
      ],
      "id": "649fb8026dbd7a08"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}